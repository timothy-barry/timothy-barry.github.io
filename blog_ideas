Research page

My interests center on 

My methodological and computational interests center on tools that I have found useful in solving applied genomics problems: low- and high-dimensional regression, latent variable models, resampling methods, and out-of-memory and distributed computing. 


Methods


Algorithms



Applications


I lead the development of several software packages. I like to think carefully about performance, user interfaces, and scalability.


Blog ideas

Genomics

1. Sc-RNA seq and UMIs
2. CRISPR
3. Lentiviruses and gene delivery
4. Single cell CRISPR sequencing approaches

Genetics

1. Meiosis and linkage disequilibrium

Statistics

1. Measure theory vocabulary
2. RMT and fundamental theorem of statistics
3. Against measure theory; use intervals rather than more general measurable sets; and use RS integration rather than more general integration. Pushforward measure; thus, in practice, working with a distribution defined on the real numbers anyway.
- In practice, we consider intervals rather than measurable sets; also, in practice, RVs have a density that is integrable.
- Simple but rigorous probability that is a subset of measure theoretic probability.
4. A map of high dimensional statistics
- Methods: penalized methods, model X
- Theoretical approaches: concentration inequalities (Wainwright, Roman), Approximate message passing theory, random matrix theory.
5. Low-D parametric straightforward
- Use MLE; exponential families and information matrices
6. High-dimensional vs low-dimensional theory in modern genomics
7. Poisson and NB regression (offsets and auxiliary parameter -- how to deal with both).
8. Models for RNA seq. Gene expression and measurement models (Sarkar + Stephens); GLM setup is a powerful way to handle RNA seq data.
9. How I personally view stats problems.
- 1) What is the setting? This is independent of our objective and method.
- 2) What is our goal?
- 3) What is our method?
- 4) How do we theoretically analyze the method?
10. Double descent
11. RMT primer
12. Computing
- What I like and dislike about R
- Like: functional (R is at its heart a functional programming language. first class functions; we can pass functions as arguments, return functions, store functions in variables, call functions with a list of arguments via do.call; basically every popular R function is side-effect free.), vectorized operations, C++ interface via Rcpp, and of course the packages, which are easy to write, install, and are diverse.
- Dislike: file paths (nightmare in R), data (which is sort of ironic for a language focused on data processing)
- I could use Python, but I find aspects of the syntax awkward and the statistical ecosystem lacking. Julia is interesting option but seems neither mature not widely used. And I would rather give up programming than use Matlab.
- My current approach, which attempts to leverage the best features of R and make up for what I see as its limitations. Use packages, but ONLY for code (not data). I have the following structure.
- R: use these for the package functions
- test: use these for tests
- drivers: Use these for "driver" files that call the package functions to carry out the analysis
- bash: Stores bash scripts and log files. Useful for metaprogramming on computer clusters. For example, running 20 driver files in parallel.
- files: I use Rprojroot. Basically, it returns a path to the base directory of the package.
- data: the "next-door-neighbor" of the package in the file system.
13. Working on compute clusters without schedulers (much of this comes from my mentor Gene)
- Unix syntax: familiarize oneself with the commands (cd, ls, mkdir, rm, etc.). Also screen is useful.
- git/github. Very useful for transferring code between machines. Make the R project (which should only store code) a GitHub repository. Git clone the repository on the server, and git pull whenever a change to the code is made.
- Simulation setup: More elaborate possibilities exist, but this is a good starting point for running simulations in an efficient way. Have one R file with all the parameter settings; the settings are enumerated from 1, ..., n. (The parameter could be hardcoded or generated by a function). A second R file that drives the situation. Basically, the file takes as a command line argument the parameter setting (and maybe the number of simulation repetitions). It then runs the simulation and saves the output. Some notes: Use "sink" to redirect the output to some appropriate file; call sink twice to redirect all output, including the messages and warnings. Save the output in some location with some file name that makes sense. Finally, we have a bash script that uses a for loop to iterate through the parameter settings and call the corresponding drivers.
