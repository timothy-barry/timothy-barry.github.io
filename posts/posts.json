[
  {
    "path": "posts/2020-07-07-generalized-linear-models/",
    "title": "Generalized linear models",
    "description": "A dive into the theory behind GLMs. This post covers the GLM model, canonical and non-canonical link functions, optimization of the log-likelihood, and inference.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-12-03",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThis post is my effort to once and for all understand GLMs. I explore various topics, including canonical and non-canonical link functions, optimization of the log likelihood, and inference.\nExponential family review\nLet \\(\\{P_\\theta\\}, \\theta \\in \\Theta \\subset \\mathbb{R}\\) be a family of distributions. Recall that \\(\\{P_\\theta\\}\\) belongs to the 1-parameter exponential family if its density can be written as \\[f(y | \\theta) = e^{ \\eta(\\theta) T(y) - \\psi(\\theta)} h(y).\\]\nWe can reparameterize the exponential family so that its density is in canonical form: \\[f(y|\\eta) = e^{ \\eta T(y) - \\psi(\\eta)}h(y).\\] In this post we will assume the data \\(\\{y_1, \\dots, y_n\\}\\) come from a slightly restricted version of the exponential family. Specifically, we will assume the \\(y_i\\)s have density \\[f(y_i | \\eta_i) = e^{\\eta_i y_i - \\psi(\\eta_i)}h(y_i),\\] i.e., we will assume the sufficient statistic \\(T(y_i)\\) is the identity. This slightly narrower class encompasses the most important distributions for statistical modeling, including the binomial, exponential, Poisson, Bernoulli, negative binomial (assuming fixed number of failures), and normal (assuming unit variance) distributions. Note that the function \\(h\\) is not a function of the unknown parameter \\(\\eta_i\\) and thus will show up as a constant in the log-likelihood. The only function that contains information (relevant to inference) about the distribution is the cumulant generating function \\(\\psi\\). Thus, we can identify exponential families (with identity sufficient statistic) with their corresponding canonical-form cumulant generating function.\nGLM model\nWe observe independent (but not necessarily identically-distributed) tuples \\(\\{(y_1, x_1), \\dots, (y_n, x_n)\\},\\) where \\(y_i \\in \\mathbb{R}\\) and \\(x_i \\in \\mathbb{R}^p\\). Assume the \\(y_i\\)s are drawn from a one-parameter exponential family with identity sufficient statistic and possibly different canonical parameters: \\[ y_i \\sim f(y_i|\\eta_i) = e^{ \\eta_i y_i - \\psi(\\eta_i)}h(y).\\] Assume the \\(x_i\\)s are fixed and known. Recall that in exponential families, there exists a bijective map between the natural parameter and the mean. Thus, the observations \\(\\{y_1, \\dots, y_n\\}\\) have means \\(\\{ \\mu_1, \\dots, \\mu_n \\}.\\)\nWe must link the responses \\(y_i\\) to the covariates \\(x_i\\). Assume there exists a strictly increasing and differentiable function \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and an unknown vector \\(\\beta \\in \\mathbb{R}^p\\) such that \\[ g(\\mu_i) = \\langle x_i, \\beta \\rangle \\] for all \\(i \\in \\{ 1, \\dots, n \\}\\). The function \\(g\\) is called the link function, and the function \\(g^{-1}\\) is called the inverse link function. With these various pieces in place, we can frame our statistical problem as follows: given data \\(\\{(y_i, x_i)\\}_{i=1}^n\\), a link function \\(g\\), and an exponential family density \\(f\\), estimate the unknown parameter \\(\\beta\\). We will estimate \\(\\beta\\) and obtain associated standard errors through MLE.\nCanonical link function\nFor a given exponential family, there exists a special link function called the canonical link function that imbues the GLM with very nice mathematical properties. Use of the canonical link is entirely optional; however, when given the option, people generally choose to use the canonical link given its nice properties.\nThe canonical link function is the link function that results from setting the linear component of the model (\\(\\langle x_i, \\beta_i \\rangle\\)) equal to the natural parameter (\\(\\eta_i\\)). In math, the canonical link function is that function \\(g_c\\) that satisfies \\[g_c(\\mu_i) = \\langle x_i, \\beta \\rangle = \\eta_i.\\] We can express the canonical link function in terms of known quantities. Recall that for an exponential family in canonical form (with identity sufficient statistic), we have \\(\\psi'(\\eta_i) = \\mathbb{E}[Y_i] = \\mu_i.\\) Therefore, by the definition of \\(g_c\\) above, we obtain \\(g_c = [\\psi']^{-1}.\\) That is, the canonical link function is equal to the inverse of the derivative of \\(\\psi\\).\nThe main advantage of using the canonical link function is that the canonical link renders the canonical parameter of the joint distribution \\(y = (y_1, \\dots, y_n)^T\\) equal to \\(\\beta\\).\nTheorem: Suppose we model the mean \\(\\mu_i\\) of \\(y_i\\) using the canonical link function, i.e. \\(g_c(\\mu_i) = \\langle x_i,\\beta \\rangle\\). Then the joint density of \\(y = (y_1, \\dots, y_n)^T\\) is a p-parameter exponential family with canonical parameter \\(\\beta\\).\nProof sketch: Define \\(\\eta = X \\beta\\), where \\(\\eta = [\\eta_1, \\dots, \\eta_n]^T \\in \\mathbb{R}^n\\) and \\(X\\) is the \\(n \\times p\\) design matrix of observations. Let \\([S_1(y), \\dots, S_p(y)]^T\\) denote the \\(p\\)-dimensional product of the matrix-vector multiplication \\(X^T y\\). Observe that \\[ \\sum_{i=1}^n \\eta_i y_i = \\sum_{i=1}^n x_i^T \\beta y_i = \\beta^T X^T y = \\langle \\beta, [S_1(y), \\dots, S_p(y)]^T \\rangle.\\] Next, define \\(\\phi: \\mathbb{R}^p \\to \\mathbb{R}\\) by \\[\\phi(\\beta) = \\sum_{i=1}^n \\psi(x_i^T \\beta) = \\sum_{i=1}^n \\psi(\\eta_i).\\] Finally, define \\[H(y) = \\prod_{i=1}^n h(y_i).\\] We can express the joint density \\(f_c\\) of \\(y = (y_1, \\dots, y_n)^T\\) as \\[f_c(y|\\beta) = e^{ \\sum_{i=1}^n \\eta_i y_i - \\psi(\\eta_i)}\\prod_{i=1}^n h(y_i) = \\left( e^{\\langle \\beta, [S_1(y), \\dots, S_p(y)]^T \\rangle - \\phi(\\beta)}\\right) H(y).\\] \\(\\square\\)\nWe see that \\(f_c\\) is a \\(p\\)-dimensional exponential family density in canonical form. We have that\n\\(\\beta\\) is the \\(p\\)-dimensional canonical parameter\n\\([S_1(y), \\dots, S_p(y)]^T\\) is the p-dimensional sufficient statistic\n\\(\\phi\\) is the cumulant generating function\n\\(H\\) is the carrier density.\nThe density \\(f_c(y|\\beta)\\) inherits all the nice properties of a \\(p\\)-dimensional exponential family density in canonical form, including convexity of the log-likelihood and equality of the observed and Fisher information matrices:\nConvexity. The log-likelihood for \\(\\beta\\) is a concave function defined over a convex set. Typically, the log-likelihood for \\(\\beta\\) is strictly concave (although this depends on \\(\\phi\\)). Thus, the MLE for \\(\\beta\\) exists, (typically) is unique, and is easy to compute.\nEquality of observed and Fisher information matrices. The observed and Fisher information matrices for \\(\\beta\\) coincide. This fact simplifies some aspects of GLM fitting and inference, as we will see.\nNon-canonical link function\nWe can use a link function that is non-canonical. Consider the same setup as before (i.e., the problem setup as presented in the GLM model section.) Given an exponential family distribution, a link function, and set of data, our goal is to estimate the unknown parameter \\(\\beta\\) of the GLM through MLE.\nWe begin by introducing some notation. Let \\(h : \\mathbb{R} \\to \\mathbb{R}\\) be the function that maps the linear predictor into the canonical parameter, i.e. \\[h( \\langle x_i, \\beta \\rangle ) = \\eta_i.\\] The function \\(h\\) is simply the identity function when we use the canonical link. In general, we can express \\(h\\) in terms of \\(g\\) and \\(\\psi\\): \\[ h( \\langle x_i, \\beta \\rangle) = \\eta_i = [\\psi']^{-1}(\\mu_i) = [\\psi']^{-1}(g^{-1}(\\langle x_i, \\beta \\rangle)).\\] Thus, \\(h = [\\psi']^{-1} \\circ g^{-1}.\\)\nAt this point we have defined a lot of functions. There are three functions in particular that are relevant to our analysis: the cumulant generating function \\(\\psi\\), the link function \\(g\\), and the above-defined function \\(h\\). We review the definitions and properties of these functions in the list below.\nFunction \\(\\psi\\)\nDefinition: the cumulant generating function of the exponential family in canonical form.\nProperties: \\(\\psi'(\\eta_i) = \\mu_i\\) and \\(\\psi''(\\eta_i) = \\sigma_i^2\\), where \\(\\sigma_i^2 = \\mathbb{V}(y_i)\\).\nFunction \\(g\\)\nDefinition: the function that maps the mean to the linear component, i.e. \\(g(\\mu_i) = \\langle x_i, \\beta \\rangle\\).\nProperties: The canonical link function \\(g_c\\) satisfies \\(g_c = [\\psi']^{-1}\\).\nFunction \\(h\\)\nDefinition: the function that maps the linear component to the canonical parameter, i.e. \\(h(\\langle x_i, \\beta \\rangle) = \\eta_i\\).\nProperties: \\(h\\) can be expressed in terms of \\(\\psi\\) and \\(g\\) as \\(h = [\\psi']^{-1} \\circ g^{-1}\\). When \\(g\\) is the canonical link, \\(h\\) is the identity.\nWith these definitions in hand, we can express the log-likelihood of the model (up to a constant) as follows: \\[\\mathcal{L}(\\beta;y,X) = \\sum_{i=1}^n \\eta_i y_i - \\psi(\\eta_i) = \\sum_{i=1}^n y_i \\cdot h(\\langle x_i, \\beta \\rangle) - \\psi( h(\\langle x_i, \\beta \\rangle)).\\] For optimization and inference purposes, we need to compute the gradient and Hessian of the log-likelihood. Recall that the gradient of the log-likelihood is the score statistic, the Hessian of the log-likelihood is the negative observed information matrix, and the expected Hessian of the log-likelihood is the negative Fisher information matrix. We need to define some matrices and vectors to express these quantities compactly. Yes, this is painful, but it is necessary. Define the matrices\n\\[\n\\begin{cases}\n\\Delta = \\textrm{diag}\\left\\{ h'( \\langle x_i, \\beta \\rangle ) \\right\\}_{i=1}^n \\\\\n\\Delta' = \\textrm{diag}\\left\\{ h''( \\langle x_i, \\beta \\rangle) \\right\\}_{i=1}^n \\\\\nV = \\textrm{diag}\\left\\{ \\psi''(\\eta_i) \\right\\}_{i=1}^n \\\\\nH = \\textrm{diag}\\left\\{ y_i - \\mu_i \\right\\}_{i=1}^n \\\\\nW = \\Delta V \\Delta.\n\\end{cases}\n\\] Also, define the vector \\(s = [ y_1 - \\mu_1, \\dots, y_n - \\mu_n]^T.\\) We can show through calculus (use the chain rule!) that \\[ \\nabla \\mathcal{L}(\\beta) = X^T \\Delta s\\] and \\[ \\nabla^2 \\mathcal{L}(\\beta) = - X^T (\\Delta V \\Delta - \\Delta'H ) X.\\]\nSuppose we use the canonical link function. Then \\(h\\) is the identity, implying \\(h'\\) is identically equal to \\(1\\) and \\(h''\\) is identically equal to \\(0\\). Thus, \\(\\Delta = I\\) and \\(\\Delta' = 0\\), yielding a simpler expression for the observed information matrix: \\[\\nabla^2 \\mathcal{L}(\\beta) = -X^TWX.\\]\nWe can compute the Fisher information matrix \\(I(\\beta)\\) by taking the expectation of the observed information matrix: \\[ I(\\beta) = - \\mathbb{E} \\left[ \\nabla^2 \\mathcal{L}(\\beta) \\right] = X^T ( \\Delta V \\Delta - \\Delta' \\mathbb{E}[H])X = X^T ( \\Delta V \\Delta)X = X^TWX.\\]\nNote that the observed and Fisher matrices coincide if we use the canonical link function (as predicted by exponential family theory).\nWe make a brief digression to discuss the log-likelihood of weighted GLMs. Let \\(T_1, \\dots, T_n > 0\\) be given scalar weights. We can generalize the log-likelihood of the GLM slightly by multiplying the \\(i\\)th term by \\(T_i\\): \\[ \\mathcal{L}(\\beta) = \\sum_{i=1}^n  T_i \\left[ y_i \\cdot h(\\langle x_i, \\beta \\rangle) - \\psi( h(\\langle x_i, \\beta \\rangle)) \\right].\\] We might do this if we consider some observations to be more “important” than others. Let \\(T = \\textrm{diag} \\{ T_i \\}_{i=1}^n\\) be the diagonal matrix of weights. It is easy to show that the gradient \\(\\nabla \\mathcal{L}\\) and Hessian \\(\\nabla^2 \\mathcal{L}\\) of the weighted log-likelihood are \\(\\nabla \\mathcal{L}(\\beta) = X^T T \\Delta s\\) and \\(\\nabla^2 \\mathcal{L}(\\beta) = - X^T T (\\Delta V \\Delta - \\Delta'H ) X\\), respectively.\nOptimization\nWe can optimize the log likelihood through one of three related methods: Newton-Raphson, Fisher scoring, or iteratively reweighted least squares. We discuss these methods seriatim.\nNewton-Raphson: Newton-Raphson is a general optimization algorithm. Let \\(f: \\mathbb{R}^p \\to \\mathbb{R}\\) be a twice continuously differentiable, concave function. The following iterative algorithm converges to the global maximum of \\(f\\): \\[ x^{(k+1)} \\leftarrow x^{(k)} - [\\nabla^2 f(x^{(k)})]^{-1} [ \\nabla f(x^{(k)})].\\] To use this algorithm to optimize \\(\\mathcal{L},\\) substitute the negative observed information matrix for \\(\\nabla^2 f(x^{(k)})\\) and the score statistic for \\(\\nabla f(x^{(k)})\\): \\[ \\beta^{(k+1)} \\leftarrow \\beta^{(k)} + \\left[ X^T (\\Delta V \\Delta - \\Delta'H ) X \\right]^{-1} X^T \\Delta s |_{\\beta = \\beta^{(k)}}.\\]\nFisher scoring: The Fisher information matrix is the expected observed information matrix. It turns out that we can replace the observed information matrix with the Fisher information matrix in the Newton-Raphson algorithm and retain global convergence. Making this substitution, we obtain \\[ \\beta^{(k+1)} \\leftarrow \\beta^{(k)} + [ X^T W X]^{-1} [ X^T \\Delta s]|_{ \\beta = \\beta^{(k)} }.\\] This modified algorithm is known as the Fisher scoring algorithm.\nIteratively reweighted least squares. We can rewrite the Fisher scoring algorithm in a clever way to derive a fast implementation of the algorithm. Define \\[\\tilde{y} = [g'(\\mu_1)y_1, \\dots, g'(\\mu_n)y_n]^T\\] and \\[\\tilde{\\mu} = [g'(\\mu_1) \\mu_1, \\dots, g'(\\mu_n)\\mu_n]^T.\\] We begin with a lemma.\nLemma: \\(\\Delta s = W(\\tilde{y} - \\tilde{\\mu})\\).\nProof: Recall that \\(g^{-1}(t) = \\psi'(h(t)).\\) By the inverse derivative theorem, \\[ \\frac{1}{g'(g^{-1}(t))} = \\psi''(h(t))h'(t).\\] Plugging in \\(\\langle X_i, \\beta \\rangle\\) for \\(t\\), we find \\[ \\begin{multline} \\frac{1}{g'(g^{-1}(\\langle X_i, \\beta \\rangle))} = \\psi''(h(\\langle X_i, \\beta \\rangle)) h'(\\langle X_i, \\beta \\rangle) \\iff \\psi''(\\eta_i) = \\frac{1}{ g'(\\mu_i) h'( \\langle X_i, \\beta \\rangle)} .\\end{multline}\\] Next, note that \\(W = \\Delta V \\Delta = \\textrm{diag}\\left\\{ h'( \\langle X_i, \\beta \\rangle )^2 \\psi''(\\eta_i) \\right\\}_{i=1}^n.\\) Plugging in the expression we derived for \\(\\psi''(\\eta_i)\\) above, we obtain \\[ W = \\textrm{diag}\\left\\{ \\frac{h'(\\langle X_i, \\beta \\rangle)}{ g'(\\mu_i) } \\right\\}_{i=1}^n.\\] Finally, it is clear that \\[\\begin{multline}\n\\Delta s = \\textrm{diag}\\{ h'(\\langle X_i, \\beta \\rangle)\\}_{i=1}^n ([ y_1 - \\mu_1, \\dots, y_n - \\mu_n]) \\\\ = \\textrm{diag}\\left\\{ \\frac{h'(\\langle X_i, \\beta \\rangle)}{ g'(\\mu_i) } \\right\\}_{i=1}^n ( g'(\\mu_1)[y_1 - \\mu_1], \\dots, g'(\\mu_n)[y_n - \\mu_n]) = W(\\tilde{y} - \\tilde{\\mu}).\n\\end{multline}\\] \\(\\square\\)\nReturning to the optimization problem, we can rewrite Fisher’s scoring algorithm as \\[ \\beta^{(k+1)} \\leftarrow \\beta^{(k)} + (X^TWX)^{-1} X^T \\Delta s\\\\ = (X^TWX)^{-1} X^TW(\\tilde{y} - \\tilde{\\mu}) \\\\ = (X^TWX)^{-1} X^TW(\\tilde{y} - \\tilde{\\mu} + X\\beta^{(k)}).\\]\nRecall the weighted least squares problem. Given an objective function \\(f: \\mathbb{R}^p \\to \\mathbb{R}\\) defined by \\[ f(\\beta) = (y - X \\beta) W (y - X\\beta)\\] for an \\(n \\times p\\) design matrix \\(X\\), a diagonal matrix of weights \\(W\\), and a response vector \\(y\\), the global minimizer is \\[ \\hat{\\beta} = (X^TWX)^{-1}X^TWy.\\] We can use a weighted least squares solver to compute \\(\\beta^{(k+1)}\\): set the design matrix equal to \\(X\\), the diagonal matrix of weights equal to \\(W\\), and the response vector equal to \\(\\tilde{y} - \\tilde{\\mu} + X\\beta^{(k)}\\). This procedure is called iteratively reweighted least squares (IRLS).\nR uses IRLS to implement the glm function. R initializes the parameter \\(\\beta^{(0)}\\) to a random value. R then repeatedly solves weighted least squares problems until the sequence \\(\\{ \\beta^{(0)}, \\beta^{(1)}, \\dots \\}\\) converges.\nInference\nWe can construct standard errors for the estimated parameter \\(\\hat{\\beta}\\) using MLE asymptotics. Recall that \\(I(\\beta)\\) is the Fisher information matrix of the model evaluated at \\(\\beta\\). If the number of samples \\(n\\) is large, we have the approximation \\[ \\hat{\\beta} \\sim N_p(\\beta, [I(\\hat{\\beta})]^{-1}).\\] We can compute \\(I(\\hat{\\beta})\\) according to the formula \\(I(\\hat{\\beta}) = X^T W(\\hat{\\beta})X,\\) where \\(W(\\hat{\\beta})\\) is the weight matrix evaluated at \\(\\hat{\\beta}\\). Note that R returns the matrix \\(W(\\hat{\\beta})\\) as part of the glm output.\nConclusion\nThis ends our three-part mini series on exponential families, information matrices, and GLMs. We saw that GLMs, though old, are an elegant, general, and powerful method for modeling and inference. In later posts we will see how GLMs can be used to model genomic and genetic data, with an eye toward single-cell and bulk-tissue RNA-seq.\n\n\n9gag\n\nReferences\nLecture Notes provided by Professor Bradley Efron.\nLecture Notes provided by Professor Philippe Rigollet.\nIbrahim, Joseph G. “Incomplete data in generalized linear models.” Journal of the American Statistical Association 85.411 (1990): 765-769.\n",
    "preview": "posts/2020-07-07-generalized-linear-models/log_reg.png",
    "last_modified": "2021-01-27T18:00:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-03-second-generation-sequencing/",
    "title": "Genomics for statisticians 2: Next-generation sequencing, polymerase chain reaction",
    "description": "DNA sequencing lies at the core of modern genomic assays. Here, we discuss next-generation sequencing and the related technology of polymerase chain reaction.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-08-11",
    "categories": [
      "Genomics"
    ],
    "contents": "\nTable of Contents\nIntroduction\nPolymerase chain reaction\nNext-generation sequencing\nReferences\nIntroduction\nDNA sequencing is the process of determining the nucleotide sequence of a DNA segment. DNA sequencing plays a prominent role in modern genomic assays. In this post we cover next-generation sequencing (also known, perhaps more accurately, as second generation sequencing). Next-generation sequencing (in contrast to first generation sequencing) is a type of sequencing based on DNA amplification and synthesis. The vast majority of sequencing data today are generated by next-generation sequencing machines. We also discuss the closely related technology of polymerase chain reaction (PCR), a method for cloning DNA. We start with PCR, as PCR is a crucial step in next-generation sequencing.\nPolymerase chain reaction\nPolymerase chain reaction (PCR) is a method for making many copies of a DNA segment.\nPCR plays an important role in DNA sequencing and other genomic technologies.\nTo perform PCR, we make a solution that contains (i) the DNA segment to be copied, (ii) a pair of DNA primers (one for the bottom strand and one for the top strand), (iii) nucleotide bases, and (iv) a DNA polymerase.\nWe then pass through the following steps (by changing the temperature of the solution from step to step):\nSeparate the two DNA strands (called denaturation).\n\nAllow the primers to bind to their complementary sequences (called annealing).\n\nAllow DNA polymerase to synthesize two complimentary strands (called extension).\n\nRepeat process.\n\n\n\n\nA schematic of PCR.\n\n\nThese steps result in the exponential growth in the number of copies of the starting DNA segment.\nSome biology notes:\nWe need DNA primers because DNA polymerase only can add bases to an existing segment of DNA.\nWe need two separate DNA primers (one for the top strand and one for the bottom strand) because DNA polymerase synthesizes DNA in the 5’ to 3’ direction only.\nTo design the primers, we must know the sequence of the DNA segment that we seek to copy (or more precisely, the sequence at both ends of the segment).\n\nNext-generation sequencing\nNext-generation sequencing is a type of DNA sequencing that involves DNA amplification and synthesis.\nThe most commonly-used protocol for next-generation sequencing the Illumina protocol, which is the one we will discuss here.\nOur objective, given a DNA segment and reference genome, is to determine from which part of the genome the DNA segment came.\nNext-generation sequencing takes place over four steps.\nStep 1: Library preparation\nA segment of DNA is isolated.\nThe segment optionally is fragmented into smaller pieces. This fragmentation facilitates parallel sequencing.\nShort, artificial DNA segments called adapters are attached (or ligated) to each DNA fragment. Adapters help drive subsequent amplification and sequencing steps. Biologists carrying out the sequencing know the nucleotide sequence of the adapters.\nThe DNA fragments are amplified through PCR.\nPrimers complimentary to the ligated adapters are used to prime the PCR. The researchers are able design such primers because they know the sequence of the adapters.\nPCR is necessary because many copies of a given DNA segment are required to sequence that segment.\n\n\n\nFigure 2: Library preparation. The pink and blue bars are short, artificial segments of DNA called adapters. Adapters contain primers for amplification and other subsequences for downstream sequencing. The DNA segment is fragmented, the adapters are ligated, and the fragments are amplified (or cloned) through PCR.\n\n\nStep 2: Cluster amplification\nThe library of DNA fragments is loaded onto a flat surface called a flow cell.\nOligonucleotides (short, artificial DNA fragments) complimentary to the adapters are glued to the surface of the flow cell, like blades of grass on a patch of soil (represented by blue and pink bars in Figure 3).\nEach DNA fragment anneals to a single bound oligonucleotide through its adapter.\nThrough a special type of PCR called bridge amplification, each bound fragment is copied many times to create a cluster of cloned fragments.\nThe advantage of bridge amplification (over standard PCR) is that (i) the cloned fragments are spatially localized, and (ii) the cloned fragments remain connected to the flow cell after bridge amplification finishes.\nAt the end of this step, we have distinct clusters of cloned fragments spread across the flow cell.\n\n\nFigure 3: Cluster amplification. Cluster amplification (in this example) produces four clusters, labeled 1-4. Fragments in a given cluster are clones of one another. Note: the color of the adapters and bound oligonucleotides in Figure 3 does not relate to the color of the adapters in Figure 2.\n\n\nStep 3: Sequencing\nSequencing is carried out by synthesis of a complimentary DNA strand (a process called sequencing by synthesis).\nSpecial nucleotides called dNTPs are used. dNTPs are like normal nucleotides but differ in two important ways.\nFirst, dNTPs have a special chemical group called a terminator. When a dNTP is incorporated into a growing DNA chain, the terminator prevents the incorporation of additional nucleotides.\nSecond, dNTPs have a florescent tag attached, with a different colored tag for each type of nucleotide.\n\nTo carry out sequencing, we pass through the following steps.\nAdd a solution of primers, DNA polymerases, and dNTP nucleotides to the flow cell.\n\nAllow the primers to anneal to the bound adapters (shown in the figure below as pink and blue bars).\n\nAllow polymerase to incorporate a dNTP into the growing chain of nucleotides. Synthesis temporarily halts because dNTP possesses a terminator.\n\nUse a laser to excite the fluorescent tag of the dNTP, causing the tag to emit light. Because each type of nucleotide possesses a differently colored tag, the color of the emitted light reveals the type of nucleotide that was just incorporated.\n\nRemove the flourecent tag and terminator of the newly added dNTP, turning the dNTP into a normal nucleotide.\n\nRepeat steps 3-5 until synthesis is complete.\n\n\nThe series of colored flashes reveals the sequence of the fragment.\nFragments in a given cluster emit the same series of colored flashes (as they share the same sequence), allowing a computer to decipher the sequence of the fragment.\nEach cluster generates a single read.\n\n\nFigure 4: Sequencing. All fragments within a cluster emit the same series of colors (as they are identical), allowing a computer to determine the sequence of the fragment. Note: the color of the adapters and bound oligonucleotides in Figure 4 do not relate to the color of the adapters in Figure 2.\n\n\nStep 4: Alignment\nThe newly-identified sequence reads are mapped onto a reference genome using bioinformatic software. This is possible because each fragment is about 350 nucleotides in length, and there are \\(4^{350} \\approx 10^{180}\\) (a huge number!) of possible DNA sequences of length 350.\nThe initial DNA segment was fragmented before amplification and synthesis. Therefore, we would expect a bunch of small fragments to map to adjacent locations of the genome. Using this information, we can deduce the region of the genome from which the original DNA segment came.\nFor a great summary of second-generation sequencing, see this video. The volume of the music at the end of the video is loud, so headphone users beware!\nReferences\nAn introduction to next-generation sequencing technology (Illumina, 2015).\nGenetics: A conceptual approach (Pierce 2014).\nImage 1 source\nImages 2 - 5 source\n",
    "preview": "posts/2020-06-03-second-generation-sequencing/next_gen_3_crop.png",
    "last_modified": "2021-01-27T18:00:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-22-exponential-families/",
    "title": "Exponential families",
    "description": "The exponential family is a mathematical abstraction that unifies common parametric probability distributions. In this post we review the definition and basic properties of exponential families.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-07-12",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThe exponential family is a mathematical abstraction that unifies common parametric probability distributions. Exponential families play a prominent role in GLMs and graphical models, two methods frequently employed in parametric statistical genomics. In this post we define exponential families and review their basic properties. We take a fairly conceptual approach, omitting proofs for the most part. This is the first post in a three-part mini series on exponential families, information matrices, and GLMs.\nOne-parameter exponential family\nA parametric model is a family of probability distributions indexed by a finite set of parameters. A one-parameter exponential family is a special type of parametric model indexed by a single scalar parameter.\nDefinition\nLet \\(X = (X_1, \\dots, X_d)\\) be a random vector with distribution \\(P_\\theta,\\) where \\(\\theta \\in \\Theta \\subset \\mathbb{R}\\). Assume the support of \\(X\\) is \\(S^d \\subset \\mathbb{R}^d\\). We say \\(\\{P_{\\theta} : \\theta \\in \\Theta \\}\\) belongs to the one-parameter exponential family if the density function \\(f\\) of \\(X\\) can be written as \\[ \\begin{equation}\\label{def}\nf(x | \\theta) = e^{ \\eta(\\theta) T(x) - \\psi(\\theta) } h(x),\n\\end{equation}\\] where \\(\\psi, \\eta : \\Theta \\to \\mathbb{R}\\) and \\(T, h : S^d \\to \\mathbb{R}\\) are functions.\nNote 1: The functions \\(\\psi, \\eta, T,\\) and \\(h\\) are non-unique.\nNote 2: Technically, we also must specify an integrator \\(\\alpha\\) with respect to which we integrate the density \\(f\\). That is, we must specify an \\(\\alpha\\) such that \\[ P(X \\in A) = \\int_{A} f \\alpha.\\] When \\(d = 1\\), \\(\\alpha(x) = x\\) for continuous distributions and \\(\\alpha(x) = \\textrm{floor}(x)\\) for discrete distributions. The integrator \\(\\alpha\\) typically is clear from the context, so we do not explicitly state it.\nThe exponential family encompasses the distributions most commonly used in statistical modeling, including the normal, exponential, gamma, beta, Bernoulli, Poisson, binomial (assuming fixed number of trials), and negative binomial (assuming fixed number of failures) distributions.\nExamples\nPoisson distribution. The density function of a Poisson distribution is \\[ f(x|\\theta) = \\frac{\\theta^x e^{-\\theta}}{x!}.\\] We can write this density as \\[ f(x|\\theta) = e^{x \\log(\\theta) -\\theta} \\frac{1}{x!}.\\] Written in this way, it is clear that \\[ \n\\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = x \\\\\n\\psi(\\theta) = \\theta \\\\\nh(x) = \\frac{1}{x!}.\n\\end{cases}\n\\] Therefore, the Poisson distribution belongs to the one-parameter exponential family.\nPoisson product distribution. Let \\(X = X_1, \\dots, X_n \\sim \\textrm{Pois}(\\theta).\\) The density function of \\(X\\) is \\[ f(x|\\theta) = \\prod_{i=1}^n \\frac{\\theta^{x_i}e^{-\\theta}}{x_i!} = \\frac{ \\theta^{\\sum_{i=1}^n x_i}e^{-n\\theta}}{\\prod_{i=1}^n x_i!}.\\] Similar to above, we can write this function as \\[ f(x|\\theta) = e^{\\log(\\theta) \\left(\\sum_{i=1}^n x_i\\right) - n\\theta} \\frac{1}{\\prod_{i=1}^n x_i!}.\\] We have \\[ \n\\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = \\sum_{i=1}^n x_i \\\\\n\\psi(\\theta) = n\\theta \\\\\nh(x) = \\frac{1}{\\prod_{i=1}^n x_i!}.\n\\end{cases}\n\\] Therefore, the Poisson product distribution, like the Poisson distribution, is a member of the one-parameter exponential family (of course, with different constituent functions).\nNegative binomial distribution. Recall the density function of a negative binomial distribution with parameters \\(r, \\theta\\) is \\[ f(x| r, \\theta) = \\binom{x + r - 1}{x} \\theta^{x}(1-\\theta)^r.\\] Assume that \\(r\\) is a known, fixed parameter. We can express the density function as \\[ f(x|r, \\theta) = e^{ \\log(\\theta) x + r\\log(1 - \\theta)} \\binom{x + r - 1}{x}.\\] Writing \\[ \\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = x \\\\\n\\psi(\\theta) = - r\\log(1 - \\theta) \\\\\nh(x) = \\binom{x + r - 1}{x},\n\\end{cases}\n\\] we see that the negative binomial distribution (with fixed \\(r\\)) is an exponential family. The \\(n-\\)fold negative binomial product distribution likewise is a one-parameter exponential family.\nProperties\nWe list several important properties of the one-parameter exponential family. The properties we state relate to sufficiency, reparameterization of the density function, convexity of the likelihood function, and moments of the distribution.\nSufficiency\nSufficiency mathematically formalizes the notion of “no loss of information.” As far as I can tell, sufficiency once played a central role in mathematical statistics but since fallen out of favor to some extent. Still, the concept of sufficiency is important to understand in the context of the exponential family.\nLet \\((X_1, \\dots, X_d)\\) be a random vector with distribution \\(P_\\theta,\\) \\(\\theta \\in \\Theta \\subset \\mathbb{R}\\). Let \\(T(X)\\) be a statistic. We call \\(T(X)\\) sufficient for \\(\\theta\\) if \\(T(X)\\) preserves all information about \\(\\theta\\) contained in \\((X_1, \\dots, X_d)\\). More precisely, \\(T(X)\\) is sufficient for \\(\\theta\\) if the distribution of \\(X\\) given \\(T(X)\\) is constant in (i.e., does not depend on) \\(\\theta\\).\nTheorem. Let \\(X = (X_1, \\dots, X_d)\\) be distributed according to the one-parameter exponential family \\(\\{P_\\theta\\}\\). Then the statistic \\(T(X)\\) is a sufficient statistic for \\(\\theta\\).\nThe proof of this fact follows easily from the Fisher–Neyman factorization theorem. Recall that \\(T(X) = \\sum_{i=1}^n X_i\\) is a sufficient statistic of the \\(n-\\)fold Poisson product distribution (see previous section). Also recall that the MLE of the Poisson distribution is \\((1/n)\\sum_{i=1}^n X_i\\). Intuitively, it makes sense that the sufficient statistic would coincide with the MLE (up to a constant). We see similar patterns for other members of the exponential family.\nReparameterization\nA common strategy in statistical analysis is to reparateterize a probability distribution. Suppose a family of probability distributions \\(\\{ P_{\\theta} \\}\\) is parameterized by \\(\\theta \\in \\mathbb{R}\\). Suppose we set \\(\\gamma = f(\\theta)\\), where \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is an invertible function. Then we can write \\(\\theta = f^{-1}(\\gamma)\\) and parameterize the family of probability distributions in terms of \\(\\gamma\\) instead of \\(\\theta\\). There is no loss of information in this reparameterization.\nConsider a family of distributions \\(\\{ P_{\\theta} : \\theta \\in \\Theta \\subset \\mathcal{R} \\}\\) that is a member of the one-parameter exponential family, i.e. that has density \\(f(x|\\theta) = e^{\\eta(\\theta)T(x) - \\psi(\\theta)}h(x).\\) Typically, the function \\(\\eta: \\Theta \\to \\mathbb{R}\\) is invertible. Therefore, we can reparameterize the family of distributions in terms of the function \\(\\eta\\).\nSet \\(\\eta^* = \\eta(\\theta)\\). Then \\(\\theta = \\eta^{-1}(\\eta^*)\\), and so we can write the density \\(f\\) as \\[f(x|\\theta) = e^{\\eta^* T(x) - \\psi( \\eta^{-1}(\\eta^*)) }h(x).\\] Setting \\(\\psi^* = \\psi \\circ \\eta^{-1},\\) we derive the reparameterization \\[ f(x|\\eta^*) = e^{ \\eta^* T(x) - \\psi^*(\\eta^*)}h(x),\\] which is parameterized in terms of \\(\\eta^*\\) rather than \\(\\theta\\). Under this new parameterization, the parameter space \\(\\mathcal{T}\\) consists of the set of values for which the density function \\(f\\) integrates to unity, i.e. \\[ \\mathcal{T} = \\{ \\eta^* \\in \\mathbb{R} : \\int_{R} e^{ \\eta^* T(x) - \\psi^*(\\eta^*)}h(x) d \\alpha(x) = 1  \\}.\\] To ease notation, we drop the asterisk (*) from \\(\\eta\\) and \\(\\psi\\). A family of probability distributions is said to be in canonical one-parameter exponential family form if its density function can be written as\\[f(x|\\eta) = e^{ \\eta T(x) - \\psi(\\eta)}h(x), \\eta \\in \\mathcal{T}.\\] The set \\(\\mathcal{T}\\) is sometimes called the natural parameter space. The canonical form of an exponential family is easy to work with mathematically. Thus, most theorems about exponential families are expressed in canonical form.\nWritten in canonical form, the terms \\(\\eta, T(x), \\psi(\\eta),\\) and \\(h(x)\\) have special names:\n\\(\\eta\\) is called the canonical (or natural) parameter,\n\\(T(x)\\) is called the sufficient statistic,\n\\(\\psi(\\eta)\\) is called the cumulant-generating function, and\n\\(h(x)\\) is called the carrying density.\nVocabulary can be a bit annoying, but in the case of exponential families it facilitates discussion. We look at a couple examples of exponential families in canonical form.\nExample: Poisson canonical form. Recall that we can write the Poisson density in exponential family form as \\[f(x | \\theta) = \\left(e^{x \\log(\\theta)} - \\theta \\right)\\frac{1}{x!}.\\] Set \\(\\eta^* = \\log(\\theta)\\). Then \\(\\theta = e^{\\eta^*},\\) and we can re-express \\(f\\) as \\[f(x| \\eta^*) = \\left(e^{x \\eta^* - e^{\\eta^*}} \\right) \\frac{1}{x!}.\\] Dropping the asterisk from \\(\\eta\\) to ease notation, we end up with the canonical parameterization \\[f(x| \\eta) = \\left(e^{x \\eta - e^{\\eta}}\\right) \\frac{1}{x!},\\] where \\(\\eta \\in \\mathcal{T} = \\mathbb{R}.\\) The canonical parameter is \\(\\eta\\), the sufficient statistic is \\(x\\), the cumulant-generating function is \\(e^\\eta\\), and the carrying density is \\(1/x!\\).\nExample: Negative binomial canonical form. We expressed the negative binomial density in exponential family form as \\[ f(x|\\theta) = e^{ \\log (\\theta)x  + r \\log(1 - \\theta)} \\binom{x + r - 1}{x}.\\] Setting \\(\\eta = \\log(\\theta)\\), we can re-write this density in canonical form as \\[ f(x|\\eta) = e^{x \\eta + r \\log(1-e^\\eta)}\\binom{x+r-1}{x}.\\] The canonical parameter is \\(\\eta\\), the sufficient statistic is \\(x\\), the cumulant-generating function is \\(r\\log(1 - e^\\eta)\\), and the carrying density is \\(\\binom{x + r - 1}{x}.\\)\nConvexity\nThe exponential family enjoys some useful convexity properties.\nTheorem: Consider a canonical exponential family with density \\[f(x|\\eta) = e^{\\eta T(x) - \\psi(\\eta)}h(x)\\] and natural parameter space \\(\\mathcal{T}\\). The set \\(\\mathcal{T}\\) is convex, and the cumulant-generating function \\(\\psi\\) is convex on \\(\\mathcal{T}\\).\nThe proof of this theorem is simple and involves the application of Holder’s inequality. This theorem has an important corollary.\nCorollary: Let \\(X = (X_1, \\dots, X_d)\\) be a random vector distributed according to the exponential family \\(\\{ P_\\eta : \\eta \\in \\mathcal{T}\\}.\\) The log-likelihood \\[\\mathcal{L}(\\eta; x) = \\log\\left(f(x|\\eta)\\right)\\] is a concave function defined on a convex set.\nThe proof of this corollary is simple. Because \\(\\psi\\) is convex, \\(-\\psi\\) is concave. The log-likelihood \\[\\mathcal{L}(\\eta;x) = \\eta T(x) - \\psi(\\eta) + \\log\\left( h(x) \\right)\\] is the sum of concave functions and is therefore concave.\nThis corollary has important implications: the MLE for \\(\\eta\\) exists and is easily computable (through convex optimization). When \\(\\psi\\) is strictly convex (which generally is the case), the MLE is unique.\nMoments\nWe easily can compute the moments of an exponential family.\nTheorem. Let \\(X = (X_1, \\dots, X_d)\\) be distributed according to the canonical exponential family \\(\\{ P_\\eta:\\eta \\in \\mathcal{T}\\}.\\) Then \\[\n\\begin{cases}\n\\mathbb{E}_\\eta [T(X)] = \\psi'(\\eta) \\\\\n\\mathbb{V}_\\eta [T(X)] = \\psi''(\\eta).\n\\end{cases}\n\\]\nWe provide a proof sketch. The density function of the expoential family integrates to unity, i.e. \\[ \\int_{\\mathbb{R}^d} e^{\\eta T(x) - \\psi(\\eta)}h(x) d\\alpha(x) = 1.\\] Therefore, \\[ e^{\\psi(\\eta)} = \\int_{ \\mathbb{R}^d } e^{\\eta T(x)}h(x) d\\alpha(x).\\] Differentiating with respect to \\(\\eta\\), we find \\[ e^{\\psi(\\eta)} \\psi'(\\eta) = \\int_{\\mathbb{R}^d} T(x)e^{\\eta T(x)} h(x) d\\alpha(x),\\] and so \\[ \\psi'(\\eta) = \\int_{\\mathbb{R}^d} T(x) e^{\\eta T(x) - \\psi(\\eta)}h(x) d\\alpha(x) = \\int_{\\mathbb{R}^d} T(x) f(x|\\eta) d\\alpha(x) = \\mathbb{E}_\\eta[T(X)].\\] Differentiating with respect to \\(\\eta\\) again and rearranging, we find \\(\\mathbb{V}_\\eta [T(X)] = \\psi''(\\eta)\\).\nExample: Recall that the Poisson distribution can be written in canonical form as \\[ f(x|\\eta) = e^{x \\eta - e^{\\eta}} \\frac{1}{x!}.\\] We have that \\(\\psi(\\eta) = e^{\\eta}\\) and \\(T(X) = X\\). Therefore, \\(\\mathbb{E}_\\eta[X] = \\psi'(\\eta) = e^{\\eta}\\) and \\(\\mathbb{V}_\\eta[X] = \\psi''(\\eta) = e^{\\eta}.\\) Recalling that \\(\\eta = \\log(\\theta)\\), we recover \\(\\mathbb{E}[X] = \\mathbb{V}[X] = \\theta\\).\nOur theorem about the moments of \\(T(X)\\) has several intriguing corollaries.\nCorollary 1. The second derivative of \\(\\psi\\) is equal to the variance of \\(T(X)\\). Because variance is non-negative, the second derivative of \\(\\psi\\) is non-negative. This implies that \\(\\psi\\) is convex. This is an alternate demonstration of the convexity of \\(\\psi\\).\nCorollary 2. Assume the variance of \\(T(X)\\) is nonzero. Then \\(\\psi\\) is strictly convex, implying \\(\\eta \\to \\mathbb{E}_\\eta[T(X)]\\) is injective. Thus, we can reparameterize the exponential family in terms of \\(\\mathbb{E}_\\eta[T(X)].\\) Because \\(T(X) = X\\) for the Poisson and negative binomial distributions in particular, we can parameterize the Poisson and negative binomial densities in terms of their means.\nMultiparameter exponential family\nWe extend the definition of the exponential family to multiparameter distributions. Results that hold for one-parameter exponential families hold analogously for multiparameter exponential families.\nDefinition\nLet the random vector \\(X = (X_1, \\dots, X_d)\\) have distribution \\(P_\\theta,\\) where \\(\\theta \\in \\Theta \\subset \\mathbb{R}^k\\). The family \\(\\{P_\\theta \\}\\) belongs to the \\(k\\)-parameter exponential family if its density can be written as \\[f(x|\\theta) = e^{ \\sum_{i=1}^k \\eta_i(\\theta)T_i(x) - \\psi(\\theta)}h(x),\\] where \\[ \\begin{cases}\n\\eta_1, \\dotsm \\eta_k : \\mathbb{R}^k \\to \\mathbb{R} \\\\\nT_1, \\dots, T_k : \\mathbb{R}^d \\to \\mathbb{R} \\\\\n\\psi : \\mathbb{R}^k \\to \\mathbb{R} \\\\\nh : \\mathbb{R}^d \\to \\mathbb{R}\n\\end{cases}\n\\] are functions, and \\(\\theta \\in \\mathbb{R}^k\\). We also require that the dimension of \\(\\theta = (\\theta_1, \\dots, \\theta_n)\\) equal the dimension of \\((\\eta_1(\\theta), \\dots, \\eta_k(\\theta))\\). (If the dimension of the latter exceeds that of the former, the distribution is said to belong to the curved exponential family.)\nExamples\nExamples of the multiparameter exponential family include the normal distribution with unknown mean and variance and generalized linear models (GLMs). We will provide more detailed examples of multiparameter exponential families in the upcoming post on GLMs.\nProperties\nWe briefly list some properties of the multiparameter exponential family related to sufficiency, reparameterization, convexity, and moments.\nSufficiency\nThe vector \\([T_1(X), \\dots, T_k(X)]^T\\) is a sufficient statistic for the parameter \\(\\theta\\). This follows from the Neyman-Fisher factorization theorem.\nReparameterization\nWe can reparameterize multivariate distributions as well. Similar to the one-parameter case, let \\[ \\begin{cases}\n\\eta_1^* = \\eta_1(\\theta_1, \\dots, \\theta_k) \\\\\n\\dots \\\\\n\\eta_k^* = \\eta_k(\\theta_1, \\dots, \\theta_k).\n\\end{cases}\n\\] Typically, we can invert this vector-valued function, i.e., we can express \\(\\theta_1, \\dots, \\theta_k\\) in terms of \\(\\eta_1^*, \\dots, \\eta_k^*\\). In this case, we can write the multiparameter exponential family in canonical form: \\[f(x|\\eta) = e^{ \\sum_{i=1}^k \\eta_i T_i(x) - \\psi(\\eta)}h(x),\\] where \\(\\eta \\in \\mathbb{R}^k, T_1, \\dots T_k: \\mathbb{R}^d \\to \\mathbb{R},\\) \\(\\psi: \\mathbb{R}^k \\to \\mathbb{R},\\) and \\(h:\\mathbb{R}^d \\to \\mathbb{R}\\). The set \\(\\mathcal{T}\\) over which the natural parameters vary is called the natural parameter space.\nConvexity\nThe natural parameter space \\(\\mathcal{T}\\) is convex, and the function \\(\\psi: \\mathbb{R}^k \\to \\mathbb{R}\\) is convex over \\(\\mathcal{T}\\). Thus, the log-likelihood for the natural parameter \\(\\eta\\) of a \\(k\\)-parameter exponential family is concave. The proof of this assertion in multiparameter families likewise leverages Holder’s inequality.\nMoments\nLet \\(X = (X_1, \\dots, X_d)\\) have distribution \\(\\{P_\\eta\\}\\) belonging to the canonical \\(k-\\)parameter exponential family. Then \\[ \\nabla \\psi(\\eta) = \\mathbb{E}_\\eta[ T_1(X), \\dots, T_k(X) ] \\] and \\[ \\nabla^2 \\psi(\\eta) = \\textrm{Cov}_\\eta[T_1(X), \\dots, T_k(X)],\\] where \\(\\nabla \\psi\\) is the gradient of \\(\\psi\\), and \\(\\nabla^2\\psi\\) is the Hessian of \\(\\psi\\). In words, the gradient of the cumulant-generating function \\(\\psi\\) is the expected value of the vector of sufficient statistics \\([T_1(X), \\dots, T_k(X)]\\), and the hessian of \\(\\psi\\) is the variance-covariance matrix of the vector of sufficient statistics. Because variance-covariance matrices are positive semi-definite, the function \\(\\psi\\) is convex. This is an alternate demonstration of the convexity of \\(\\psi\\). The Hessian of \\(\\psi\\) evaulated at \\(\\eta\\) sometimes is called the Fisher information matrix (evaluated at \\(\\eta\\)).\nConclusion\nThe exponential family is a mathematical abstraction that unifies common parametric probability distributions. In this post we defined exponential families and explored some of their basic properties. In the remaining two posts of this mini-series, we will explore the connection between exponential families, information matrices, and GLMs.\n\n\nme.me\n\nReferenes\nLecture notes provided by Professor Anirban DasGupta.\n",
    "preview": "posts/2020-06-22-exponential-families/happy_normal.jpg",
    "last_modified": "2021-01-27T18:00:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-16-gamma-poisson-nb/",
    "title": "Gamma, Poisson, and negative binomial distributions",
    "description": "The gamma, Poisson, and negative binomial distributions are used extensively in genomics. In this post we review these distributions and their connections to one another. We also cover the various (and somewhat confusing) parameterizations of these distributions.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-06-16",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThe gamma, Poisson, and negative binomial distributions frequently are used to model RNAseq and single cell RNAseq data. Here, we review the statistical properties of these distributions. We postpone discussion of modeling RNAseq data to a subsequent post.\nGamma\nThe gamma distribution is a non-negative, continuous, two-parameter probability distribution. There are two common parameterizations of the gamma distribution: the “shape-scale” parameterization and the “shape-rate” parameterization. The pdf of the gamma distribution under the former parameterization is \\[f(x;k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} e^{-\\frac{x}{\\theta}},\\] where \\(k > 0\\) is the shape parameter, \\(\\theta > 0\\) is the scale parameter, and \\(\\Gamma\\) is the gamma function. Recall that, for positive real numbers, the gamma function is defined by \\[ \\Gamma(x) = \\int_{0}^\\infty t^{-x}e^{-t} dt.\\] The Gamma distribution has support \\((0,\\infty)\\). The mean and variance of the Gamma distribution (under shape-scale parameterization) are \\(k\\theta\\) and \\(k\\theta^2\\). The alternate parameterization of the gamma distribution — the shape-rate parameterization — sets \\(\\alpha = k\\) and \\(\\beta = 1/\\theta\\). The parameter \\(\\alpha\\) retains the name of the shape parameter \\(k\\), and \\(\\beta\\) is called the scale parameter.\nPoisson\nThe Poisson distribution is a discrete probability distribution used to model (non-negative) count data. The pmf of the Poisson distribution is \\[ p(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!},\\] where \\(\\lambda > 0\\) is called the rate parameter. The support of the distribution is \\(\\mathbb{Z}^{\\geq 0}\\), and the mean and variance are \\(\\lambda\\). The Poisson and Gamma distributions are members of the exponential family, and so parameter estimation (through, e.g., MLE) is simple in both models.\nNegative binomial\nThe negative binomial (NB) distribution is a discrete probability distribution that takes support on the non-negative integers. The NB distribution models the number of failures in a sequence of independent trials before a specified number of successes occurs. For example, suppose we flip a coin repeatedly until we see 10 heads. The total number of tails we see throughout the experiment follows an NB distribution. The pmf of the NB distribution (under standard parameterization) is \\[ p(x; r, p) = \\binom{x + r - 1}{x} (1 - p)^r p^x, \\] where \\(r \\in \\mathbb{Z}^{\\geq 0}\\) is the number of successes, and \\(p \\in [0,1]\\) is the probability of failure. The mean and variance of the NB distribution are \\[\\frac{pr}{1-p}\\] and \\[\\frac{1 + p}{(1-p)^2}.\\] The pmf of the NB distribution can be generalized to allow for \\(r \\in \\mathbb{R}^{\\geq 0}\\) as follows: \\[ p(x; r, p) = \\frac{\\Gamma(x + r)}{x! \\Gamma(r)}(1-p)^rp^k.\\] This extension follows from properties of the gamma function.\nIf one holds \\(r\\) fixed, the NB distribution is a member of the exponential family. However, when both \\(r\\) and \\(p\\) are allowed to vary, the NB distribution is not member of the exponential family. This means that parameter estimation in the two-parameter NB model is a bit more challenging than parameter estimation in many other common parameteric models. In particular, the NB maximum likelihood estimate fails to exist when the sample mean exceeds the sample seond moment (Aragon, Eberly, and Eberly 1992) .\nGamma-Poisson mixture\nA very cool (and useful) fact about the negative binomial distribution is that it can be written as a mixture of gamma and Poisson distributions. Consider a Poisson model with gamma-distributed mean: \\[\n\\begin{cases}\nY \\sim \\textrm{Pois}(\\theta) \\\\\n\\theta \\sim \\textrm{gamma}(r, \\frac{p}{1-p}),\n\\end{cases}\n\\] where the gamma distribution is expressed in shape-scale parameterization. One can show that \\(Y\\) is negative binomially distributed with parameters \\(r\\) and \\(p\\), i.e. \\[ Y \\sim \\textrm{NB}(r,p).\\] A quick proof (taken from this blog post) is as follows: \\[\n\\begin{align*}\np(y; r, p) = \\int_{0}^{\\infty} p(y | \\theta) p(\\theta) d \\theta \\textrm{ (law of total probability)} \\\\\n= \\int_{0}^{\\infty} \\left( \\frac{\\theta^y e^{-\\theta}}{y!} \\right) \\left( \\frac{ \\theta^{r-1} e^{-\\theta (1 - p)/p} }{ \\Gamma(r) \\left(\\frac{p}{1-p}\\right)^r}\\right) d\\theta \\textrm{ (plug in pdfs)} \n\\\\ = \\frac{\\Gamma(y + r)}{y! \\Gamma(r)}(1-p)^r p^y \\textrm{ (compute integral)} \\\\ \\sim NB(r,p).\n\\end{align*}\n\\]\nParameterizations of the negative binomial distribution\nThere are several parameterizations of the negative binomial distribution. We list three such parameterizations in the table below. The left column shows the (shape-scale) parameterization of the underlying gamma distribution. The middle and right columns show the parameterization and pmf of the corresponding NB distribution.\nIdx\nGamma parameterization\nNB parameterization\nNB pmf\n1.\nGamma\\(\\left(r, \\frac{p}{1 - p}\\right)\\)\nNB\\(\\left(r,p\\right)\\)\n\\[\\binom{x + r - 1}{x} p^x (1-p)^r \\]\n2.\nGamma\\(\\left(r, \\frac{p}{r} \\right)\\)\nNB\\(\\left(r, \\frac{p}{p+r}\\right)\\)\n\\[ \\binom{x + r - 1}{x} \\left( \\frac{p}{p+r} \\right)^x \\left( \\frac{r}{p+r} \\right)^r  \\]\n3.\nGamma\\(\\left(r, p \\right)\\)\nNB\\(\\left(r, \\frac{p}{p+1}\\right)\\)\n\\[ \\binom{x + r - 1}{x} \\left( \\frac{p}{p+1} \\right)^x\\left( \\frac{1}{p+1} \\right)^r \\]\nOf these three NB parameterizations, the most common are the first and second. The first is similar (but not identical) to the default used by the various NB functions (dnbinom, rnbinom, etc.) in base R (see documentation). The second is the default used by the function negbinomial in the popular R package VGAM. Note that in the second and third parameterizations, \\(p\\) takes values in \\(\\mathbb{R}^{\\geq 0}\\) and therefore cannot be interpreted as a probability.\nNB, Poisson, and overdispersion\nConsider the second parameterization of the NB distribution in the table above. Its mean is \\[ \\frac{p/(p+r)r}{1 -p/(p+r)} = p\\] and its variance is \\[ \\frac{ p/(p+r) r }{ (1 - p/(p+r))^2 } = p + \\frac{p^2}{r}.\\] Thus, for \\(r < \\infty\\), the variance of the negative binomial distribution exceeds the mean. This is an important difference between the Poisson and NB distributions — in the Poisson distribution, the mean and variance coincide. One can show that the NB distribution converges to the Poisson distribution as \\(r\\) tends to infinity. Informally, \\[\\lim_{r \\to \\infty} \\textrm{NB}\\left( r, \\frac{\\lambda}{r + \\lambda}\\right) = \\textrm{Pois}(\\lambda).\\] Thus, for large \\(r\\), the NB and Poisson distributions are approximately equivalent. For small \\(r\\), the NB distribution is a sort of “overdispersed” Poisson distribution. This property of the NB distribution makes it an appealing candidate for modeling highly variable gene expression data.\n\n\nAragon, Jorge, David Eberly, and Shelly Eberly. 1992. “Existence and uniqueness of the maximum likelihood estimator for the two-parameter negative binomial distribution.” Statistics and Probability Letters 15 (5): 375–79. https://doi.org/10.1016/0167-7152(92)90157-Z.\n\n\n\n\n",
    "preview": "posts/2020-06-16-gamma-poisson-nb/picsvg.png",
    "last_modified": "2021-01-27T18:00:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-27-enhancers/",
    "title": "Genomics for statisticians 1: Enhancers",
    "description": "Enhancers are segments of DNA that increase the expression of a nearby gene. This post provides an overview enhancer biology and detection of enhancers at scale.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-05-27",
    "categories": [
      "Genomics"
    ],
    "contents": "\nTable of Contents\nIntroduction\nReview of chromatin and transcription\nReview of enhancer biology\nMethods for enhancer identification at scale\nCRISPR-based methods for enhancer identification at scale\nReferences\nThis is the first post in a series called Genomics for Statisticians. In this series I will explore some important ideas in modern genomics from the perspective of a non-biologist. When I began research in statistical genomics, I quickly discovered that my knowledge of genomics was not quite up to par. This series is the result of my effort to fill that gap. I hope other researchers in genomics who do not have extensive formal training in biology (e.g., statisticians, computer scientists) will find this series helpful as well. The posts should be reasonably self-contained. My goal is to explore concepts at the level of a college biology class, roughly. The first post in the series in on enhancers.\nIntroduction\nEnhancers are short regions of DNA that increase the expression of a nearby gene.\nMost variants responsible for heritability fall outside genes and in enhancers. Therefore, enhancers are important to study and understand.\nThis post has three parts. First, we review necessary background information on chromatin and transcription. Second, we review enhancer biology. Finally, we review methods for identifying enhancers and linking enhancers to genes at scale.\nReview of chromatin and transcription\nStructure of DNA, chromatin, and chromosomes\nDNA is a double-stranded molecule that stores genetic information. A strand of DNA consists of a bunch of linked nucleotides. Nucleotides come in four types: A, T, G, C.\nDNA molecules do not exist on their own in the nuclei of cells. Instead, they are packaged with proteins called histones to form chromatin. Put simply, chromatin = DNA + histones.\nThere are 5 histone proteins: H1, H2A, H2B, H3, and H4.\nA nucleosome is a chromatin structure consisting of a DNA strand wrapped twice around a nucleosome core, anchored in place by an H1 protein. A nucleosome core consists of two copies each of H2A, H2B, H3, and H4.\n\n\nA nucleosome, which consists of an 8-histone core, a strand of DNA wrapped twice around the histone core, and an H1 histone to help anchor the DNA in place.\n\n\nNucleosomes pack together in a highly organized way to form chromosomes.\n\n\n\nNucleosomes are arranged to form chromosomes.\n\n\nChanges in chromatin structure\nDNA in tightly-coiled chromatin cannot be accessed by proteins.\nChromatin can “loosen up,” thereby exposing DNA to proteins.\nChromatin “loosens” or “tightens” by three main processes:\nChromatin remodeling: Proteins called chromatin remodeling complexes physically reposition nucleosomes to expose DNA.\n\nHistone modification: Histones are chemically altered by addition or removal of functional groups.\nExample: an acetyl group (CH\\(_3\\)CO) can be added to the 27th lysine amino acid of H3 to loosen the chromatin. In chemical notation, we write H3 \\(\\to\\) H3K27ac (K stands for “lysine,” and “ac” stands for acetyl group).\nExample: a methyl group (CH\\(_3\\)) can be added to the 4th lysine amino acid of H3 to loosen the chromatin. In chemical notation, we write H3 \\(\\to\\) H3K4me1 (“me1” stands for methyl group).\n\nDNA methylation: Cytosine bases of DNA also can be methylated to form 5-methylcytosine. Heavily methylated DNA is generally not transcribed.\n\n\nTranscription of genes\nTranscription is the copying of a gene into a complimentary RNA molecule.\nA promoter is a short sequence of DNA upstream of a gene that helps initiate transcription.\nThe protein polymerase and other proteins called transcription factors and transcription activator proteins bind to the promoter.\nLike a train traveling down railroad tracks, polymerase travels from the promoter into the gene and synthesizes a complimentary strand of RNA.\nReview of enhancer biology\nWhat is an enhancer?\nAn enhancer is a short sequence of DNA that increases the transcription of one or more nearby genes through physical contact (known as a cis-regulatory mechanism).\n\nHow do enhancers work?\nEnhancers bind transcription factors and transcription activator proteins and bring these proteins close (in physical space) to the promoter of the target gene. This action modulates the expression of the target gene.\nEnhancers can regulate expression of their target gene in other ways. For example, some enhancers regulate the release of paused polymerase, and others act through RNA splicing mechanisms.\n\n\n\n\nAn enhancer modulating the expression of a gene.\n\n\nWhat are some features of enhancers?\nEnhancers are located in regions of open (or “loose”) chromatin.\nEnhancers are flanked by histones carrying H3K27ac and H3K4me1 modifications.\nEnhancers are flanked by methylated DNA.\nEnhancers are a few hundred base pairs in length and act over \\(\\approx\\) 30,000 bases.\nGenes can be affected by a single enhancer or multiple, “interacting” enhancers; conversely, individual enhancers can regulate multiple genes.\nEnhancers can reside in clusters of hundreds of enhancers, forming “super enhancers.”\n\nMethods for enhancer identification at scale\nDNA sequence analysis: Enhancers harbor transcription factor binding motifs (i.e., very short stretches of DNA that help bind transcription factors). Furthermore, enhancers are often conserved across species. Thus, we can try to predict whether a region of the genome is an enhancer simply by looking at the corresponding primary sequence.\nBiochemical annotations: As we have discussed, several biochemical annotations correlate with enhancer activity. Genome-wide, we can search for (i) histone modifications (e.g., presence H3K27ac and H3K4me1), (ii) transcription factor binding, (iii) open chromatin, (iv) DNA methylation, and (v) the initiation of transcription. Many assays, both bulk-tissue and single-cell, provide us with this information (see, for example, DNase-seq, Pro-seq, and ChIP-seq).\neQTL mapping: For a given SNP and given nearby gene, we can test whether expression of that gene differs significantly across the levels of the SNP. If so, the SNP may lie within an enhancer for that gene. A downside of this approach is that it operates at the resolution of linkage disequilibrium blocks.\n3D conformation mapping: There exist assays (e.g., Hi-C) to probe the 3D conformation (in space) of a chromosome. If a given region of DNA is in close proximity to a known promoter, then that region might be an enhancer.\nCRISPR-based approaches: See next section.\nCRISPR-based methods for enhancer identification at scale\nCRISPR-Cas9 is a flexible technology for genome perturbation and editing.\nThe basic idea is to use CRISPR to “perturb” (in some way) a short sequence of DNA. If this perturbation is associated with a shift in the expression of a nearby gene, then the targeted sequence of DNA is likely an enhancer for that gene.\nCRISPR allows us to link enhancers to their target genes and (possibly) establish rigorous causal relationships between enhancer activity and gene expression.\nCRISPR-based approaches differ along two main axes:\nNumber of genes measured: Some CRISPR-based protocols track only a single gene; others track all genes (through, e.g., scRNA-seq).\n\nMode of perturbation: CRISPR can “perturb” a candidate enhancer in several ways:\nCRISPR–Cas9 (nuclease active) makes a single cut inside the enhancer. The cell uses non-homologous end joining to repair the cut. Generally, a few extra bases are inserted or deleted during this process, altering and consequently deactivating the targeted enhancer.\nCRISPRi (short for CRISPR interference) “turns off” a candidate enhancer without altering its associated DNA sequence.\n\n\nReferences\nToward a comprehensive catalog of validated and target-linked human enhancers (Gasperini, Tome, and Shendure 2020).\nGenetics: A conceptual approach (Pierce 2014).\nImage 1 source.\nImage 2 source.\nImage 3 source.\n",
    "preview": "posts/2020-05-27-enhancers/Nucleosome.jpg",
    "last_modified": "2021-01-27T18:00:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "About this blog",
    "description": "Statistics, genetics, and (maybe) memes",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-05-27",
    "categories": [
      "About"
    ],
    "contents": "\nHello world,\nWelcome to my blog. This is my first blog post ever (hi Mom). I intend to use this blog to cover topics in statistics, genetics, and genomics. Lots of people use these terms in different ways, so maybe I should start by defining these terms myself.\nStatistics: The science and practice of drawing insights from datasets. Statistics has two main objectives: understanding relationships between variables and making predictions.\nGenetics: The study of how traits are passed from parent to offspring and from generation to generation.\nGenomics: The biochemical study of gene expression and regulation.\n\n\n\nFigure 1. This figure does not convey any useful information. Left: the Greek letter sigma for statistics; right: a strand of DNA for genetics and genomics.\n\n How do statistics, genetics, and genomics fit together? Biological technologies developed over the past decade have given rise to very large, whole-genome genetic and genomic datasets. To pull something useful out of these data, researchers need to leverage up-to-date statistical and machine learning tools. Conversely, modern biological datasets have inspired the development of new statistical theory, especially in the areas of high-dimensional inference and prediction. In this blog I will explore current ideas at the intersection of genetics, genomics, and statistics, such as CRISPR-Cas9 genome editing, single-cell RNA-seq, and lentiviral gene delivery. Topics for the most part will be drawn from my own research, but I hope other scientists working in this space will find the posts helpful. My goal is to post about once a month.\nAnd yes, there may be memes.\n\n\nFigure 2. Courtesy of Statistical Statistics Memes.\n\n\n\n",
    "preview": "posts/welcome/pic.png",
    "last_modified": "2021-01-27T18:00:19-05:00",
    "input_file": {}
  }
]
