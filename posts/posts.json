[
  {
    "path": "posts/2020-07-07-generalized-linear-models/",
    "title": "Generalized linear models",
    "description": "A dive into the theory behind GLMs. This post covers the GLM model, canonical and non-canonical link functions, optimization of the log-likelihood, and inference.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2023-01-10",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThis post is my effort to once and for all understand GLMs. I explore\nvarious topics, including canonical and non-canonical link functions,\noptimization of the log likelihood, and inference.\nExponential family review\nLet \\(\\{P_\\theta\\}, \\theta \\in \\Theta\n\\subset \\mathbb{R}\\) be a family of distributions. Recall that\n\\(\\{P_\\theta\\}\\) belongs to the\n1-parameter exponential family if its density can be written as \\[f(y|\\theta) = e^{ \\theta T(y) -\n\\psi(\\theta)}h(y),\\] where \\(\\theta\\) is called the canonical parameter.\nIn this post we will assume the data \\(\\{y_1,\n\\dots, y_n\\}\\) come from a slightly restricted version of the\nexponential family. Specifically, we will assume the \\(y_i\\)s have density \\[f(y_i | \\theta_i) = e^{\\theta_i y_i -\n\\psi(\\theta_i)}h(y_i),\\] i.e., we will assume the sufficient\nstatistic \\(T(y_i)\\) is the identity.\nThis slightly narrower class encompasses the most important\ndistributions for statistical modeling, including the binomial,\nexponential, Poisson, Bernoulli, negative binomial (assuming fixed\nnumber of failures), and normal (assuming unit variance) distributions.\nNote that the function \\(h\\) is not a\nfunction of the unknown parameter \\(\\theta_i\\) and thus will show up as a\nconstant in the log-likelihood. The only function that contains\ninformation (relevant to inference) about the distribution is the\ncumulant generating function \\(\\psi\\).\nThus, we can identify exponential families (with identity sufficient\nstatistic) with their corresponding canonical-form cumulant generating\nfunction.\nGLM model\nWe observe independent (but not necessarily identically-distributed)\ntuples \\(\\{(y_1, x_1), \\dots, (y_n,\nx_n)\\},\\) where \\(y_i \\in\n\\mathbb{R}\\) and \\(x_i \\in\n\\mathbb{R}^p\\). Assume the \\(y_i\\)s are drawn from a one-parameter\nexponential family with identity sufficient statistic and possibly\ndifferent canonical parameters: \\[ y_i \\sim\nf(y_i|\\theta_i) = e^{ \\theta_i y_i - \\psi(\\theta_i)}h(y).\\]\nAssume the \\(x_i\\)s are fixed and\nknown. Recall that in exponential families, there exists a bijective map\nbetween the natural parameter and the mean. Thus, the observations \\(\\{y_1, \\dots, y_n\\}\\) have means \\(\\{ \\mu_1, \\dots, \\mu_n \\}.\\)\nWe must link the responses \\(y_i\\)\nto the covariates \\(x_i\\). Assume there\nexists a strictly increasing and differentiable function \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and an\nunknown vector \\(\\beta \\in\n\\mathbb{R}^p\\) such that \\[ g(\\mu_i) =\n\\langle x_i, \\beta \\rangle \\] for all \\(i \\in \\{ 1, \\dots, n \\}\\). The function\n\\(g\\) is called the link\nfunction, and the function \\(g^{-1}\\) is called the inverse link\nfunction. We sometimes use the symbol \\(\\eta_i\\) to denote the linear component of\nthe model, i.e. \\(\\eta_i = \\langle x_i, \\beta\n\\rangle\\). With these various pieces in place, we can frame our\nstatistical problem as follows: given data \\(\\{(y_i, x_i)\\}_{i=1}^n\\), a link function\n\\(g\\), and an exponential family\ndensity \\(f\\), estimate the unknown\nparameter \\(\\beta\\). We will estimate\n\\(\\beta\\) and obtain associated\nstandard errors through MLE.\nCanonical link function\nFor a given exponential family, there exists a special link function\ncalled the canonical link function that imbues the GLM\nwith very nice mathematical properties. Use of the canonical link is\nentirely optional; however, when given the option, people generally\nchoose to use the canonical link given its nice properties.\nThe canonical link function is the link function that results from\nsetting the linear component of the model \\(\\langle x_i, \\beta_i \\rangle\\) equal to the\nnatural parameter (\\(\\theta_i\\)). In\nmath, the canonical link function is that function \\(g_c\\) that satisfies \\[g_c(\\mu_i) = \\langle x_i, \\beta \\rangle =\n\\theta_i.\\] We can express the canonical link function in terms\nof known quantities. Recall that for an exponential family in canonical\nform (with identity sufficient statistic), we have \\(\\psi'(\\theta_i) = \\mathbb{E}[Y_i] =\n\\mu_i.\\) Therefore, by the definition of \\(g_c\\) above, we obtain \\(g_c = [\\psi']^{-1}.\\) That is, the\ncanonical link function is equal to the inverse of the derivative of\n\\(\\psi\\).\nThe main advantage of using the canonical link function is that the\ncanonical link renders the canonical parameter of the joint distribution\n\\(y = (y_1, \\dots, y_n)^T\\) equal to\n\\(\\beta\\).\nTheorem: Suppose we model the mean \\(\\mu_i\\) of \\(y_i\\) using the canonical link function,\ni.e. \\(g_c(\\mu_i) = \\langle x_i,\\beta\n\\rangle\\). Then the joint density of \\(y = (y_1, \\dots, y_n)^T\\) is a p-parameter\nexponential family with canonical parameter \\(\\beta\\).\nProof sketch: Define \\(\\theta = X \\beta\\), where \\(\\theta = [\\theta_1, \\dots, \\theta_n]^T \\in\n\\mathbb{R}^n\\) and \\(X\\) is the\n\\(n \\times p\\) design matrix of\nobservations. Let \\([S_1(y), \\dots,\nS_p(y)]^T\\) denote the \\(p\\)-dimensional product of the\nmatrix-vector multiplication \\(X^T y\\).\nObserve that \\[ \\sum_{i=1}^n \\theta_i y_i =\n\\sum_{i=1}^n x_i^T \\beta y_i = \\beta^T X^T y = \\langle \\beta, [S_1(y),\n\\dots, S_p(y)]^T \\rangle.\\] Next, define \\(\\phi: \\mathbb{R}^p \\to \\mathbb{R}\\) by\n\\[\\phi(\\beta) = \\sum_{i=1}^n \\psi(x_i^T\n\\beta) = \\sum_{i=1}^n \\psi(\\theta_i).\\] Finally, define \\[H(y) = \\prod_{i=1}^n h(y_i).\\] We can\nexpress the joint density \\(f_c\\) of\n\\(y = (y_1, \\dots, y_n)^T\\) as \\[f_c(y|\\beta) = e^{ \\sum_{i=1}^n \\theta_i y_i -\n\\psi(\\theta_i)}\\prod_{i=1}^n h(y_i) = \\left( e^{\\langle \\beta, [S_1(y),\n\\dots, S_p(y)]^T \\rangle - \\phi(\\beta)}\\right) H(y).\\] \\(\\square\\)\nWe see that \\(f_c\\) is a \\(p\\)-dimensional exponential family density\nin canonical form. We have that\n\\(\\beta\\) is the \\(p\\)-dimensional canonical parameter\n\\([S_1(y), \\dots, S_p(y)]^T\\) is\nthe p-dimensional sufficient statistic\n\\(\\phi\\) is the cumulant generating\nfunction\n\\(H\\) is the carrier density.\nThe density \\(f_c(y|\\beta)\\)\ninherits all the nice properties of a \\(p\\)-dimensional exponential family density\nin canonical form, including convexity of the log-likelihood and\nequality of the observed and Fisher information matrices:\nConvexity. The log-likelihood for \\(\\beta\\) is a concave function defined over\na convex set. Typically, the log-likelihood for \\(\\beta\\) is strictly concave (although this\ndepends on \\(\\phi\\)). Thus, the MLE for\n\\(\\beta\\) exists, (typically) is\nunique, and is easy to compute.\nEquality of observed and Fisher information\nmatrices. The observed and Fisher information matrices for\n\\(\\beta\\) coincide. This fact\nsimplifies some aspects of GLM fitting and inference, as we will\nsee.\nNon-canonical link function\nWe can use a link function that is non-canonical. Consider the same\nsetup as before (i.e., the problem setup as presented in the GLM\nmodel section.) Given an exponential family distribution, a link\nfunction, and set of data, our goal is to estimate the unknown parameter\n\\(\\beta\\) of the GLM through MLE.\nWe begin by introducing some notation. Let \\(h : \\mathbb{R} \\to \\mathbb{R}\\) be the\nfunction that maps the linear predictor into the canonical parameter,\ni.e. \\[h( \\langle x_i, \\beta \\rangle ) =\n\\theta_i.\\] The function \\(h\\)\nis simply the identity function when we use the canonical link. In\ngeneral, we can express \\(h\\) in terms\nof \\(g\\) and \\(\\psi\\): \\[ h(\n\\langle x_i, \\beta \\rangle) = \\theta_i = [\\psi']^{-1}(\\mu_i) =\n[\\psi']^{-1}(g^{-1}(\\langle x_i, \\beta \\rangle)).\\] Thus,\n\\(h = [\\psi']^{-1} \\circ\ng^{-1}.\\)\nAt this point we have defined a lot of functions. There are three\nfunctions in particular that are relevant to our analysis: the cumulant\ngenerating function \\(\\psi\\), the link\nfunction \\(g\\), and the above-defined\nfunction \\(h\\). We review the\ndefinitions and properties of these functions in the list below.\nFunction \\(\\psi\\)\nDefinition: the cumulant generating function of the exponential\nfamily in canonical form.\nProperties: \\(\\psi'(\\theta_i) =\n\\mu_i\\) and \\(\\psi''(\\theta_i)\n= \\sigma_i^2\\), where \\(\\sigma_i^2 =\n\\mathbb{V}(y_i)\\).\nFunction \\(g\\)\nDefinition: the function that maps the mean to the linear component,\ni.e. \\(g(\\mu_i) = \\langle x_i, \\beta\n\\rangle\\).\nProperties: The canonical link function \\(g_c\\) satisfies \\(g_c = [\\psi']^{-1}\\).\nFunction \\(h\\)\nDefinition: the function that maps the linear component to the\ncanonical parameter, i.e. \\(h(\\langle x_i,\n\\beta \\rangle) = \\theta_i\\).\nProperties: \\(h\\) can be expressed\nin terms of \\(\\psi\\) and \\(g\\) as \\(h =\n[\\psi']^{-1} \\circ g^{-1}\\). When \\(g\\) is the canonical link, \\(h\\) is the identity.\nWith these definitions in hand, we can express the log-likelihood of\nthe model (up to a constant) as follows: \\[\\mathcal{L}(\\beta;y,X) = \\sum_{i=1}^n \\theta_i\ny_i - \\psi(\\theta_i) = \\sum_{i=1}^n y_i \\cdot h(\\langle x_i, \\beta\n\\rangle) - \\psi( h(\\langle x_i, \\beta \\rangle)).\\] For\noptimization and inference purposes, we need to compute the gradient and\nHessian of the log-likelihood. Recall that the gradient of the\nlog-likelihood is the score statistic, the Hessian of the log-likelihood\nis the negative observed information matrix, and the expected Hessian of\nthe log-likelihood is the negative Fisher information matrix. We need to\ndefine some matrices and vectors to express these quantities compactly.\nYes, this is painful, but it is necessary. Define the matrices\n\\[\n\\begin{cases}\n\\Delta = \\textrm{diag}\\left\\{ h'( \\langle x_i, \\beta \\rangle )\n\\right\\}_{i=1}^n \\\\\n\\Delta' = \\textrm{diag}\\left\\{ h''( \\langle x_i, \\beta\n\\rangle) \\right\\}_{i=1}^n \\\\\nV = \\textrm{diag}\\left\\{ \\psi''(\\theta_i) \\right\\}_{i=1}^n \\\\\nH = \\textrm{diag}\\left\\{ y_i - \\mu_i \\right\\}_{i=1}^n \\\\\nW = \\Delta V \\Delta.\n\\end{cases}\n\\] Also, define the vector \\(s = [ y_1\n- \\mu_1, \\dots, y_n - \\mu_n]^T.\\) We can show through calculus\n(use the chain rule!) that \\[ \\nabla\n\\mathcal{L}(\\beta) = X^T \\Delta s\\] and \\[ \\nabla^2 \\mathcal{L}(\\beta) = - X^T (\\Delta V\n\\Delta - \\Delta'H ) X.\\]\nSuppose we use the canonical link function. Then \\(h\\) is the identity, implying \\(h'\\) is identically equal to \\(1\\) and \\(h''\\) is identically equal to \\(0\\). Thus, \\(\\Delta = I\\) and \\(\\Delta' = 0\\), yielding a simpler\nexpression for the observed information matrix: \\[\\nabla^2 \\mathcal{L}(\\beta) =\n-X^TWX.\\]\nWe can compute the Fisher information matrix \\(I(\\beta)\\) by taking the expectation of the\nobserved information matrix: \\[ I(\\beta) = -\n\\mathbb{E} \\left[ \\nabla^2 \\mathcal{L}(\\beta) \\right] = X^T ( \\Delta V\n\\Delta - \\Delta' \\mathbb{E}[H])X = X^T ( \\Delta V \\Delta)X =\nX^TWX.\\]\nNote that the observed and Fisher matrices coincide if we use the\ncanonical link function (as predicted by exponential family theory).\nWe make a brief digression to discuss the log-likelihood of weighted\nGLMs. Let \\(T_1, \\dots, T_n > 0\\) be\ngiven scalar weights. We can generalize the log-likelihood of the GLM\nslightly by multiplying the \\(i\\)th\nterm by \\(T_i\\): \\[ \\mathcal{L}(\\beta) = \\sum_{i=1}^n  T_i \\left[\ny_i \\cdot h(\\langle x_i, \\beta \\rangle) - \\psi( h(\\langle x_i, \\beta\n\\rangle)) \\right].\\] We might do this if we consider some\nobservations to be more “important” than others. Let \\(T = \\textrm{diag} \\{ T_i \\}_{i=1}^n\\) be\nthe diagonal matrix of weights. It is easy to show that the gradient\n\\(\\nabla \\mathcal{L}\\) and Hessian\n\\(\\nabla^2 \\mathcal{L}\\) of the\nweighted log-likelihood are \\(\\nabla\n\\mathcal{L}(\\beta) = X^T T \\Delta s\\) and \\(\\nabla^2 \\mathcal{L}(\\beta) = - X^T T (\\Delta V\n\\Delta - \\Delta'H ) X\\), respectively.\nOptimization\nWe can optimize the log likelihood through one of three related\nmethods: Newton-Raphson, Fisher scoring, or iteratively reweighted least\nsquares. We discuss these methods seriatim.\nNewton-Raphson: Newton-Raphson is a general\noptimization algorithm. Let \\(f: \\mathbb{R}^p\n\\to \\mathbb{R}\\) be a twice continuously differentiable, concave\nfunction. The following iterative algorithm converges to the global\nmaximum of \\(f\\): \\[ x^{(k+1)} \\leftarrow x^{(k)} - [\\nabla^2\nf(x^{(k)})]^{-1} [ \\nabla f(x^{(k)})].\\] To use this algorithm to\noptimize \\(\\mathcal{L},\\) substitute\nthe negative observed information matrix for \\(\\nabla^2 f(x^{(k)})\\) and the score\nstatistic for \\(\\nabla f(x^{(k)})\\):\n\\[ \\beta^{(k+1)} \\leftarrow \\beta^{(k)} +\n\\left[ X^T (\\Delta V \\Delta - \\Delta'H ) X \\right]^{-1} X^T \\Delta s\n|_{\\beta = \\beta^{(k)}}.\\]\nFisher scoring: The Fisher information matrix is the\nexpected observed information matrix. It turns out that we can replace\nthe observed information matrix with the Fisher information matrix in\nthe Newton-Raphson algorithm and retain global convergence. Making this\nsubstitution, we obtain \\[ \\beta^{(k+1)}\n\\leftarrow \\beta^{(k)} + [ X^T W X]^{-1} [ X^T \\Delta s]|_{ \\beta =\n\\beta^{(k)} }.\\] This modified algorithm is known as the Fisher\nscoring algorithm.\nIteratively reweighted least squares. We can rewrite\nthe Fisher scoring algorithm in a clever way to derive a fast\nimplementation of the algorithm. Define \\[\\tilde{y} = [g'(\\mu_1)y_1, \\dots,\ng'(\\mu_n)y_n]^T\\] and \\[\\tilde{\\mu} = [g'(\\mu_1) \\mu_1, \\dots,\ng'(\\mu_n)\\mu_n]^T.\\] We begin with a lemma.\nLemma: \\(\\Delta s =\nW(\\tilde{y} - \\tilde{\\mu})\\).\nProof: Recall that \\(g^{-1}(t) = \\psi'(h(t)).\\) By the\ninverse derivative theorem, \\[\n\\frac{1}{g'(g^{-1}(t))} = \\psi''(h(t))h'(t).\\]\nPlugging in \\(\\langle X_i, \\beta\n\\rangle\\) for \\(t\\), we find\n\\[ \\begin{multline}\n\\frac{1}{g'(g^{-1}(\\langle X_i, \\beta \\rangle))} =\n\\psi''(h(\\langle X_i, \\beta \\rangle)) h'(\\langle X_i, \\beta\n\\rangle) \\iff \\psi''(\\theta_i) = \\frac{1}{ g'(\\mu_i) h'(\n\\langle X_i, \\beta \\rangle)} .\\end{multline}\\] Next, note that\n\\(W = \\Delta V \\Delta = \\textrm{diag}\\left\\{\nh'( \\langle X_i, \\beta \\rangle )^2 \\psi''(\\theta_i)\n\\right\\}_{i=1}^n.\\) Plugging in the expression we derived for\n\\(\\psi''(\\theta_i)\\) above, we\nobtain \\[ W = \\textrm{diag}\\left\\{\n\\frac{h'(\\langle X_i, \\beta \\rangle)}{ g'(\\mu_i) }\n\\right\\}_{i=1}^n.\\] Finally, it is clear that \\[\\begin{multline}\n\\Delta s = \\textrm{diag}\\{ h'(\\langle X_i, \\beta \\rangle)\\}_{i=1}^n\n([ y_1 - \\mu_1, \\dots, y_n - \\mu_n]) \\\\ = \\textrm{diag}\\left\\{\n\\frac{h'(\\langle X_i, \\beta \\rangle)}{ g'(\\mu_i) }\n\\right\\}_{i=1}^n ( g'(\\mu_1)[y_1 - \\mu_1], \\dots, g'(\\mu_n)[y_n\n- \\mu_n]) = W(\\tilde{y} - \\tilde{\\mu}).\n\\end{multline}\\] \\(\\square\\)\nReturning to the optimization problem, we can rewrite Fisher’s\nscoring algorithm as \\[ \\beta^{(k+1)}\n\\leftarrow \\beta^{(k)} + (X^TWX)^{-1} X^T \\Delta s\\\\ = (X^TWX)^{-1}\nX^TW(\\tilde{y} - \\tilde{\\mu}) \\\\ = (X^TWX)^{-1} X^TW(\\tilde{y} -\n\\tilde{\\mu} + X\\beta^{(k)}).\\]\nRecall the weighted least squares problem. Given an objective\nfunction \\(f: \\mathbb{R}^p \\to\n\\mathbb{R}\\) defined by \\[ f(\\beta) =\n(y - X \\beta) W (y - X\\beta)\\] for an \\(n \\times p\\) design matrix \\(X\\), a diagonal matrix of weights \\(W\\), and a response vector \\(y\\), the global minimizer is \\[ \\hat{\\beta} = (X^TWX)^{-1}X^TWy.\\] We can\nuse a weighted least squares solver to compute \\(\\beta^{(k+1)}\\): set the design matrix\nequal to \\(X\\), the diagonal matrix of\nweights equal to \\(W\\), and the\nresponse vector equal to \\(\\tilde{y} -\n\\tilde{\\mu} + X\\beta^{(k)}\\). This procedure is called\niteratively reweighted least squares (IRLS).\nR uses IRLS to implement the glm function. R initializes the\nparameter \\(\\beta^{(0)}\\) to a random\nvalue. R then repeatedly solves weighted least squares problems until\nthe sequence \\(\\{ \\beta^{(0)}, \\beta^{(1)},\n\\dots \\}\\) converges.\nAlternate notation\nSometimes, the score vector is expressed using slightly different\nnotation. Define the function \\(V\\) by\n\\(V(\\mu_i) := \\mathbb{V}(y_i),\\) i.e.,\n\\(V\\) takes as an argument the mean\n\\(\\mu_i\\) and outputs the variance of\n\\(\\mathbb{V}(y_i)\\) of \\(y_i\\). Given that \\(\\mu_i = \\psi'(\\theta_i)\\), we can use\nimplicit differentiation to express \\(V(\\mu_i)\\) as follows:\n\\[V(\\mu_i) = \\frac{d\\mu_i}{d\\theta_i} =\n\\psi''(\\theta_i) = \\mathbb{V}(y_i),\\] i.e., \\(V(\\mu_i)\\) is the differential of \\(\\mu_i\\) with respect to \\(\\theta_i\\). Next, recalling that \\(g(\\mu_i) = \\eta_i\\) (where \\(\\eta_i\\)) is the linear component of the\nmodel, we consider the following, additional implicit derivative: \\[ \\frac{d \\eta_i}{d \\mu_i} =\ng'(\\mu_i).\\] Using this alternate notation, we can express\nthe diagonal entries of the weight matrix \\(W\\) as follows: \\[W_i =\n\\frac{1}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}.\\] This expression for \\(W_i\\) coincides with our previous\nexpression for \\(W_i\\), i.e., \\[W_i = h'(\\eta_i)^2\n\\psi''(\\theta_i).\\] This holds for the following reason:\nrecall from the previous lemma that \\[\\psi''(\\theta_i) = \\frac{1}{g'(\\mu_i)\nh'(\\eta_i)}.\\] Thus,\n\\[\\psi''(\\theta_i) =\n\\frac{1}{g'(\\mu_i) h'(\\eta_i)} \\iff \\psi''(\\theta_i)^2 =\n\\frac{1}{g'(\\mu_i)^2 h'(\\eta_i)^2} \\] \\[ \\iff \\frac{1}{\\psi''(\\theta_i)\ng'(\\mu_i)^2} = h'(\\eta_i)^2 \\psi''(\\theta_i) \\iff\n\\frac{1}{ V(\\mu_i) (d \\eta_i/d \\mu_i)^2} =\nh'(\\eta_i)^2\\psi''(\\theta_i).\\] Define the matrix\n\\(M\\) by \\(\\textrm{diag} \\left\\{ d\\eta_i/d\\mu_i\n\\right\\}_{i=1}^n.\\) Then we can express the score vector as \\[ \\nabla \\mathcal{L}(\\beta) = X^T WM (y -\n\\mu),\\] because \\[W_i M_i =\n\\frac{1}{V(\\mu_i) (d\\eta_i/d\\mu_i)} = h'(\\eta_i) =\n\\Delta_i,\\] again by the lemma. Under this formulation the Fisher\ninformation matrix \\(I(\\beta) = X^T W\nX\\) remains unchanged.\nUsing this alternate notation, the \\(r\\)th Fisher scoring iteration can be\nwritten as \\[\\hat{\\beta}^{(r+1)} \\leftarrow\n\\hat{\\beta}^{(r)} + (X^T W X)^{-1}X^T W M (y - \\hat{\\mu}),\\]\nwhere all terms on the right hand side have been evaluated at \\(\\beta = \\hat{\\beta}^{(r)}\\). Define the\n\\(r\\)th “working response vector” \\(z^{(r)}\\) as \\[z^{(r)} = \\hat{\\eta}^{(r)} + M(y -\n\\hat{\\mu}^{(r)}) = X \\hat{\\beta}^{(r)} + M(y -\n\\hat{\\mu}^{(r)}).\\] We can write the \\(r\\)th scoring iteration in terms of \\(z^{(r)}\\) as follows:\n\\[ \\hat{\\beta}^{(r)} + (X^TWX)^{-1}X^TWM(y\n- \\hat{\\mu}^{(r)}) = (X^TWX)^{-1}(X^TWX) \\hat{\\beta}^{(r)} +\n(X^TWX)^{-1}X^TW M(y - \\hat{\\mu}^{(r)})\\] \\[=  (X^TWX)^{-1} X^TW ( X \\hat{\\beta}^{(r)} + M(y\n- \\hat{\\mu}^{(r)}))  = (X^TWX)^{-1} X^TWz.\\]\nThe \\(r\\)th iteration of the\nprocedure therefore reduces to the least squares problem \\[ \\hat{\\beta}^{(r+1)} \\leftarrow\n(X^TWX)^{-1}X^TWz^{(r)}.\\] We proceed until we converge upon the\nMLE \\(\\hat{\\beta}\\), recomputing the\nweight matrix \\(W\\) and the working\nresponse vector \\(z\\) at each step\nusing our updated estimates for \\(\\beta\\).\nInference\nWald\nWe can obtain Wald standard errors and \\(p\\)-values for the estimated parameter\n\\(\\hat{\\beta}\\) using MLE asymptotics.\nRecall that \\(I(\\beta)\\) is the Fisher\ninformation matrix of the model evaluated at \\(\\beta\\). If the number of samples \\(n\\) is large, we have the approximation\n\\[ \\hat{\\beta} \\sim N_p(\\beta,\n[I(\\hat{\\beta})]^{-1}).\\] We can compute \\(I(\\hat{\\beta})\\) according to the formula\n\\(I(\\hat{\\beta}) = X^T\nW(\\hat{\\beta})X,\\) where \\(W(\\hat{\\beta})\\) is the weight matrix\nevaluated at \\(\\hat{\\beta}\\). Note that\nR returns the matrix \\(W(\\hat{\\beta})\\)\nas part of the glm output.\nScore\nWe can use a score test to test whether a new predictor should be\nadded to an existing (i.e., already-fitted) GLM. Let \\(z\\), \\(W\\), and \\(\\hat{\\eta}\\) denote the final (converged)\nvalues of the working response vector, weight matrix, and linear\ncomponent vector, respectively. Let the “working residuals” \\(e = [e_1, \\dots, e_n]\\) be defined by \\[e_i = z_i - \\hat{\\eta}_i,\\] i.e., \\(e_i\\) is the \\(i\\)th residual of the weighted least\nsquares regression of \\(z\\) onto the\ncolumns of \\(X\\) using weights \\(W\\). Let \\(X_2\n\\in \\mathbb{R}^n\\) be a new column of predictors. Let \\(E_2\\) be the vector of residuals that we\nobtain after regressing \\(X_2\\) on the\ncolumns of \\(X\\) with weights \\(W\\): \\[E_2 :=\nX_2 - X(X^T W X)^{-1} X^T W X_2.\\] The z-score test statistic is\nthen\n\\[Z = \\frac{E_2^T W e}{\\sqrt{E_2^T W\nE_2}}.\\] Under the null hypothesis \\(Z\n\\sim N(0,1)\\) for large \\(n\\).\nNote that \\(e\\) is only a function of\nthe fitted GLM (and thus remains unchanged if we decide to test new\nvectors \\(X_2\\)). \\(E_2\\), by contrast, is a function of both\nthe fitted GLM and the new vector of predictors.\nIn R we can obtain the working residuals vector \\(e\\) from a fitted GLM object\nfit via fit$residuals; likewise, we can obtain\nthe working weights \\(W\\) via\nfit$weights. We can use the function\nglm.scoretest() from the package statmod\nto run a score test on a fitted GLM.\nConclusion\nThis ends our three-part mini series on exponential families,\ninformation matrices, and GLMs. We saw that GLMs, though old, are an\nelegant, general, and powerful method for modeling and inference. In\nlater posts we will see how GLMs can be used to model genomic and\ngenetic data, with an eye toward single-cell and bulk-tissue\nRNA-seq.\n\n\n9gag\n\nReferences\nLecture\nNotes provided by Professor Bradley Efron.\nLecture\nNotes provided by Professor Philippe Rigollet.\nIbrahim, Joseph G. “Incomplete\ndata in generalized linear models.” Journal of the American\nStatistical Association 85.411 (1990): 765-769.\n\n\n\n",
    "preview": "posts/2020-07-07-generalized-linear-models/log_reg.png",
    "last_modified": "2023-01-10T14:26:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-small-and-large-scale-single-cell-sequencing/",
    "title": "Genomics for Statisticians 3: Small- and large-scale single-cell sequencing",
    "description": "We overview single-cell RNA sequencing, at both small and large scales.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2022-08-15",
    "categories": [
      "Genomics"
    ],
    "contents": "\n1. Single-cell sequencing\nSingle-cell sequencing is a technology that provides a snapshot of\nthe transcriptome (i.e., the set of gene expressions) of a collection of\nindividual cells. We present a brief ovewview of single-cell\nsequencing.\n\n\na) Start with some tissue or collection of cells.\nOften, the tissue consists of cells of many types, and the goal is to\nuse single-cell sequencing to identify those cell types.\n(b) Generate lots of barcoded beads. Each barcoded\nbead consists of two parts: (i) a bead, and (ii) a short oligonucleotide\n(or “oligo”). The oligos are tethered to the bead, like hairs on a\nperson’s head.\nThe oligo is made of up several smaller parts.\nPCR handle – this is required for the PCR and sequencing steps; it\nis common across all oligos on all beads.\nCell barcode – this identifes the cell; importantly, the cell\nbarcode is the same across all oligos on a given bead.\nUMI (unique molecular identifier) – each oligo on a given bead has a\ndistinct UMI. The UMI eliminates bias that arises during the PCR and\nsequencing steps.\nPoly-dt sequece – a long sequence of T nucleotides. This captures\nthe mRNA transcripts, which are polyadenylated after transcription.\n(c) Using a microfluidic device, trap each cell\ninside a droplet along with a barcoded bead.\n(d) Lyse the cells within the droplets.\n(e) The mRNAs bind the oligos. Use reverse\ntranscription to generate cDNAs hybridized to the bead surface.\n(f) Wash the cDNAs away from the beads, and\nsequence in bulk (see this\npost for a review of sequencing).\n(g) Use software (e.g., cellranger) to generate\nthe cell-by-gene expression matrix. This roughly involves (i) collapsing\nall reads with the same UMI into a single read, (ii) determining the\ncell from which a read came, and (iii) mapping the read onto a reference\ngenome to determine the gene from which the read came.\n2. Large-scale single-cell\nsequencing\nThere is a major limitation to standard (or small-scale)\nsingle-cell sequencing: each droplet can contain at most one cell. If\nmultiple cells are captured in a single droplet, then their\ntranscriptomes are impossible to distinguish.\nExperimenters do not have exact control over how many cells are\ncaptured in a droplet; this quantity is Poisson-distributed. By\nexperimental design, the majority of droplets contain zero\ncells.\n\n\nA new strategy called scifi-RNA-seq rectifies this\ninefficiency.\n\n\nThe cells are divided into groups on a microwell plate (i.e., a\nplate with a bunch of wells). The membrane of each cell is permeabilized\n(not lysed), and the entire transcriptome is barcoded with a round-1\nbarcode and reverse transcribed into cDNA. A UMI and PCR handle are also\nadded during this step. In the figure, the round-1 barcodes are shown in\ncolor.\n\nUsing a microfluidic device, capture multiple cells inside a droplet\nalong with a barcoded bead.\n\nLyse the cells inside the droplets.\n\nLigate a round-2 barcode onto the cDNAs. In the figure, the round-2\nbarcodes are represented as shapes.\n\nPool together the cDNAs and sequence them in bulk. Use software to\ngenerate the cell-by-gene expression matrix. Use both the round-1\nand round-2 barcodes to determine the cell from which a\ntranscript came.\n\n3. Applications\nLarge-scale single-cell sequencing enables exciting new\napplications in single-cell CRISPR screens, cell atlases, and population\ngenomics.\nsci-RNA-seq3 (a method related to\nscifi-RNA-seq) was used to generate an atlas of human fetal\ntissue.\n\n\nLarge-scale single-cell sequencing opens new opportunities for\nstatisticians, data scientists, and computer scientists.\n\n\n\n",
    "preview": "posts/2021-03-19-small-and-large-scale-single-cell-sequencing/human-fetal.png",
    "last_modified": "2022-08-15T12:00:30-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-03-second-generation-sequencing/",
    "title": "Genomics for statisticians 2: Next-generation sequencing, polymerase chain reaction",
    "description": "DNA sequencing lies at the core of modern genomic assays. Here, we discuss next-generation sequencing and the related technology of polymerase chain reaction.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-08-11",
    "categories": [
      "Genomics"
    ],
    "contents": "\nTable of Contents\nIntroduction\nPolymerase chain reaction\nNext-generation sequencing\nReferences\nIntroduction\nDNA sequencing is the process of determining the nucleotide sequence of a DNA segment. DNA sequencing plays a prominent role in modern genomic assays. In this post we cover next-generation sequencing (also known, perhaps more accurately, as second generation sequencing). Next-generation sequencing (in contrast to first generation sequencing) is a type of sequencing based on DNA amplification and synthesis. The vast majority of sequencing data today are generated by next-generation sequencing machines. We also discuss the closely related technology of polymerase chain reaction (PCR), a method for cloning DNA. We start with PCR, as PCR is a crucial step in next-generation sequencing.\nPolymerase chain reaction\nPolymerase chain reaction (PCR) is a method for making many copies of a DNA segment.\nPCR plays an important role in DNA sequencing and other genomic technologies.\nTo perform PCR, we make a solution that contains (i) the DNA segment to be copied, (ii) a pair of DNA primers (one for the bottom strand and one for the top strand), (iii) nucleotide bases, and (iv) a DNA polymerase.\nWe then pass through the following steps (by changing the temperature of the solution from step to step):\nSeparate the two DNA strands (called denaturation).\n\nAllow the primers to bind to their complementary sequences (called annealing).\n\nAllow DNA polymerase to synthesize two complimentary strands (called extension).\n\nRepeat process.\n\n\n\n\nA schematic of PCR.\n\n\nThese steps result in the exponential growth in the number of copies of the starting DNA segment.\nSome biology notes:\nWe need DNA primers because DNA polymerase only can add bases to an existing segment of DNA.\nWe need two separate DNA primers (one for the top strand and one for the bottom strand) because DNA polymerase synthesizes DNA in the 5’ to 3’ direction only.\nTo design the primers, we must know the sequence of the DNA segment that we seek to copy (or more precisely, the sequence at both ends of the segment).\n\nNext-generation sequencing\nNext-generation sequencing is a type of DNA sequencing that involves DNA amplification and synthesis.\nThe most commonly-used protocol for next-generation sequencing the Illumina protocol, which is the one we will discuss here.\nOur objective, given a DNA segment and reference genome, is to determine from which part of the genome the DNA segment came.\nNext-generation sequencing takes place over four steps.\nStep 1: Library preparation\nA segment of DNA is isolated.\nThe segment optionally is fragmented into smaller pieces. This fragmentation facilitates parallel sequencing.\nShort, artificial DNA segments called adapters are attached (or ligated) to each DNA fragment. Adapters help drive subsequent amplification and sequencing steps. Biologists carrying out the sequencing know the nucleotide sequence of the adapters.\nThe DNA fragments are amplified through PCR.\nPrimers complimentary to the ligated adapters are used to prime the PCR. The researchers are able design such primers because they know the sequence of the adapters.\nPCR is necessary because many copies of a given DNA segment are required to sequence that segment.\n\n\n\nFigure 2: Library preparation. The pink and blue bars are short, artificial segments of DNA called adapters. Adapters contain primers for amplification and other subsequences for downstream sequencing. The DNA segment is fragmented, the adapters are ligated, and the fragments are amplified (or cloned) through PCR.\n\n\nStep 2: Cluster amplification\nThe library of DNA fragments is loaded onto a flat surface called a flow cell.\nOligonucleotides (short, artificial DNA fragments) complimentary to the adapters are glued to the surface of the flow cell, like blades of grass on a patch of soil (represented by blue and pink bars in Figure 3).\nEach DNA fragment anneals to a single bound oligonucleotide through its adapter.\nThrough a special type of PCR called bridge amplification, each bound fragment is copied many times to create a cluster of cloned fragments.\nThe advantage of bridge amplification (over standard PCR) is that (i) the cloned fragments are spatially localized, and (ii) the cloned fragments remain connected to the flow cell after bridge amplification finishes.\nAt the end of this step, we have distinct clusters of cloned fragments spread across the flow cell.\n\n\nFigure 3: Cluster amplification. Cluster amplification (in this example) produces four clusters, labeled 1-4. Fragments in a given cluster are clones of one another. Note: the color of the adapters and bound oligonucleotides in Figure 3 does not relate to the color of the adapters in Figure 2.\n\n\nStep 3: Sequencing\nSequencing is carried out by synthesis of a complimentary DNA strand (a process called sequencing by synthesis).\nSpecial nucleotides called dNTPs are used. dNTPs are like normal nucleotides but differ in two important ways.\nFirst, dNTPs have a special chemical group called a terminator. When a dNTP is incorporated into a growing DNA chain, the terminator prevents the incorporation of additional nucleotides.\nSecond, dNTPs have a florescent tag attached, with a different colored tag for each type of nucleotide.\n\nTo carry out sequencing, we pass through the following steps.\nAdd a solution of primers, DNA polymerases, and dNTP nucleotides to the flow cell.\n\nAllow the primers to anneal to the bound adapters (shown in the figure below as pink and blue bars).\n\nAllow polymerase to incorporate a dNTP into the growing chain of nucleotides. Synthesis temporarily halts because dNTP possesses a terminator.\n\nUse a laser to excite the fluorescent tag of the dNTP, causing the tag to emit light. Because each type of nucleotide possesses a differently colored tag, the color of the emitted light reveals the type of nucleotide that was just incorporated.\n\nRemove the flourecent tag and terminator of the newly added dNTP, turning the dNTP into a normal nucleotide.\n\nRepeat steps 3-5 until synthesis is complete.\n\n\nThe series of colored flashes reveals the sequence of the fragment.\nFragments in a given cluster emit the same series of colored flashes (as they share the same sequence), allowing a computer to decipher the sequence of the fragment.\nEach cluster generates a single read.\n\n\nFigure 4: Sequencing. All fragments within a cluster emit the same series of colors (as they are identical), allowing a computer to determine the sequence of the fragment. Note: the color of the adapters and bound oligonucleotides in Figure 4 do not relate to the color of the adapters in Figure 2.\n\n\nStep 4: Alignment\nThe newly-identified sequence reads are mapped onto a reference genome using bioinformatic software. This is possible because each fragment is about 350 nucleotides in length, and there are \\(4^{350} \\approx 10^{180}\\) (a huge number!) of possible DNA sequences of length 350.\nThe initial DNA segment was fragmented before amplification and synthesis. Therefore, we would expect a bunch of small fragments to map to adjacent locations of the genome. Using this information, we can deduce the region of the genome from which the original DNA segment came.\nFor a great summary of second-generation sequencing, see this video. The volume of the music at the end of the video is loud, so headphone users beware!\nReferences\nAn introduction to next-generation sequencing technology (Illumina, 2015).\nGenetics: A conceptual approach (Pierce 2014).\nImage 1 source\nImages 2 - 5 source\n",
    "preview": "posts/2020-06-03-second-generation-sequencing/next_gen_3_crop.png",
    "last_modified": "2021-05-21T21:30:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-22-exponential-families/",
    "title": "Exponential families",
    "description": "The exponential family is a mathematical abstraction that unifies common parametric probability distributions. In this post we review the definition and basic properties of exponential families.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-07-12",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThe exponential family is a mathematical abstraction that unifies common parametric probability distributions. Exponential families play a prominent role in GLMs and graphical models, two methods frequently employed in parametric statistical genomics. In this post we define exponential families and review their basic properties. We take a fairly conceptual approach, omitting proofs for the most part. This is the first post in a three-part mini series on exponential families, information matrices, and GLMs.\nOne-parameter exponential family\nA parametric model is a family of probability distributions indexed by a finite set of parameters. A one-parameter exponential family is a special type of parametric model indexed by a single scalar parameter.\nDefinition\nLet \\(X = (X_1, \\dots, X_d)\\) be a random vector with distribution \\(P_\\theta,\\) where \\(\\theta \\in \\Theta \\subset \\mathbb{R}\\). Assume the support of \\(X\\) is \\(S^d \\subset \\mathbb{R}^d\\). We say \\(\\{P_{\\theta} : \\theta \\in \\Theta \\}\\) belongs to the one-parameter exponential family if the density function \\(f\\) of \\(X\\) can be written as \\[ \\begin{equation}\\label{def}\nf(x | \\theta) = e^{ \\eta(\\theta) T(x) - \\psi(\\theta) } h(x),\n\\end{equation}\\] where \\(\\psi, \\eta : \\Theta \\to \\mathbb{R}\\) and \\(T, h : S^d \\to \\mathbb{R}\\) are functions.\nNote 1: The functions \\(\\psi, \\eta, T,\\) and \\(h\\) are non-unique.\nNote 2: Technically, we also must specify an integrator \\(\\alpha\\) with respect to which we integrate the density \\(f\\). That is, we must specify an \\(\\alpha\\) such that \\[ P(X \\in A) = \\int_{A} f \\alpha.\\] When \\(d = 1\\), \\(\\alpha(x) = x\\) for continuous distributions and \\(\\alpha(x) = \\textrm{floor}(x)\\) for discrete distributions. The integrator \\(\\alpha\\) typically is clear from the context, so we do not explicitly state it.\nThe exponential family encompasses the distributions most commonly used in statistical modeling, including the normal, exponential, gamma, beta, Bernoulli, Poisson, binomial (assuming fixed number of trials), and negative binomial (assuming fixed number of failures) distributions.\nExamples\nPoisson distribution. The density function of a Poisson distribution is \\[ f(x|\\theta) = \\frac{\\theta^x e^{-\\theta}}{x!}.\\] We can write this density as \\[ f(x|\\theta) = e^{x \\log(\\theta) -\\theta} \\frac{1}{x!}.\\] Written in this way, it is clear that \\[ \n\\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = x \\\\\n\\psi(\\theta) = \\theta \\\\\nh(x) = \\frac{1}{x!}.\n\\end{cases}\n\\] Therefore, the Poisson distribution belongs to the one-parameter exponential family.\nPoisson product distribution. Let \\(X = X_1, \\dots, X_n \\sim \\textrm{Pois}(\\theta).\\) The density function of \\(X\\) is \\[ f(x|\\theta) = \\prod_{i=1}^n \\frac{\\theta^{x_i}e^{-\\theta}}{x_i!} = \\frac{ \\theta^{\\sum_{i=1}^n x_i}e^{-n\\theta}}{\\prod_{i=1}^n x_i!}.\\] Similar to above, we can write this function as \\[ f(x|\\theta) = e^{\\log(\\theta) \\left(\\sum_{i=1}^n x_i\\right) - n\\theta} \\frac{1}{\\prod_{i=1}^n x_i!}.\\] We have \\[ \n\\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = \\sum_{i=1}^n x_i \\\\\n\\psi(\\theta) = n\\theta \\\\\nh(x) = \\frac{1}{\\prod_{i=1}^n x_i!}.\n\\end{cases}\n\\] Therefore, the Poisson product distribution, like the Poisson distribution, is a member of the one-parameter exponential family (of course, with different constituent functions).\nNegative binomial distribution. Recall the density function of a negative binomial distribution with parameters \\(r, \\theta\\) is \\[ f(x| r, \\theta) = \\binom{x + r - 1}{x} \\theta^{x}(1-\\theta)^r.\\] Assume that \\(r\\) is a known, fixed parameter. We can express the density function as \\[ f(x|r, \\theta) = e^{ \\log(\\theta) x + r\\log(1 - \\theta)} \\binom{x + r - 1}{x}.\\] Writing \\[ \\begin{cases}\n\\eta(\\theta) = \\log(\\theta) \\\\\nT(x) = x \\\\\n\\psi(\\theta) = - r\\log(1 - \\theta) \\\\\nh(x) = \\binom{x + r - 1}{x},\n\\end{cases}\n\\] we see that the negative binomial distribution (with fixed \\(r\\)) is an exponential family. The \\(n-\\)fold negative binomial product distribution likewise is a one-parameter exponential family.\nProperties\nWe list several important properties of the one-parameter exponential family. The properties we state relate to sufficiency, reparameterization of the density function, convexity of the likelihood function, and moments of the distribution.\nSufficiency\nSufficiency mathematically formalizes the notion of “no loss of information.” As far as I can tell, sufficiency once played a central role in mathematical statistics but since fallen out of favor to some extent. Still, the concept of sufficiency is important to understand in the context of the exponential family.\nLet \\((X_1, \\dots, X_d)\\) be a random vector with distribution \\(P_\\theta,\\) \\(\\theta \\in \\Theta \\subset \\mathbb{R}\\). Let \\(T(X)\\) be a statistic. We call \\(T(X)\\) sufficient for \\(\\theta\\) if \\(T(X)\\) preserves all information about \\(\\theta\\) contained in \\((X_1, \\dots, X_d)\\). More precisely, \\(T(X)\\) is sufficient for \\(\\theta\\) if the distribution of \\(X\\) given \\(T(X)\\) is constant in (i.e., does not depend on) \\(\\theta\\).\nTheorem. Let \\(X = (X_1, \\dots, X_d)\\) be distributed according to the one-parameter exponential family \\(\\{P_\\theta\\}\\). Then the statistic \\(T(X)\\) is a sufficient statistic for \\(\\theta\\).\nThe proof of this fact follows easily from the Fisher–Neyman factorization theorem. Recall that \\(T(X) = \\sum_{i=1}^n X_i\\) is a sufficient statistic of the \\(n-\\)fold Poisson product distribution (see previous section). Also recall that the MLE of the Poisson distribution is \\((1/n)\\sum_{i=1}^n X_i\\). Intuitively, it makes sense that the sufficient statistic would coincide with the MLE (up to a constant). We see similar patterns for other members of the exponential family.\nReparameterization\nA common strategy in statistical analysis is to reparateterize a probability distribution. Suppose a family of probability distributions \\(\\{ P_{\\theta} \\}\\) is parameterized by \\(\\theta \\in \\mathbb{R}\\). Suppose we set \\(\\gamma = f(\\theta)\\), where \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is an invertible function. Then we can write \\(\\theta = f^{-1}(\\gamma)\\) and parameterize the family of probability distributions in terms of \\(\\gamma\\) instead of \\(\\theta\\). There is no loss of information in this reparameterization.\nConsider a family of distributions \\(\\{ P_{\\theta} : \\theta \\in \\Theta \\subset \\mathcal{R} \\}\\) that is a member of the one-parameter exponential family, i.e. that has density \\(f(x|\\theta) = e^{\\eta(\\theta)T(x) - \\psi(\\theta)}h(x).\\) Typically, the function \\(\\eta: \\Theta \\to \\mathbb{R}\\) is invertible. Therefore, we can reparameterize the family of distributions in terms of the function \\(\\eta\\).\nSet \\(\\eta^* = \\eta(\\theta)\\). Then \\(\\theta = \\eta^{-1}(\\eta^*)\\), and so we can write the density \\(f\\) as \\[f(x|\\theta) = e^{\\eta^* T(x) - \\psi( \\eta^{-1}(\\eta^*)) }h(x).\\] Setting \\(\\psi^* = \\psi \\circ \\eta^{-1},\\) we derive the reparameterization \\[ f(x|\\eta^*) = e^{ \\eta^* T(x) - \\psi^*(\\eta^*)}h(x),\\] which is parameterized in terms of \\(\\eta^*\\) rather than \\(\\theta\\). Under this new parameterization, the parameter space \\(\\mathcal{T}\\) consists of the set of values for which the density function \\(f\\) integrates to unity, i.e. \\[ \\mathcal{T} = \\{ \\eta^* \\in \\mathbb{R} : \\int_{R} e^{ \\eta^* T(x) - \\psi^*(\\eta^*)}h(x) d \\alpha(x) = 1  \\}.\\] To ease notation, we drop the asterisk (*) from \\(\\eta\\) and \\(\\psi\\). A family of probability distributions is said to be in canonical one-parameter exponential family form if its density function can be written as\\[f(x|\\eta) = e^{ \\eta T(x) - \\psi(\\eta)}h(x), \\eta \\in \\mathcal{T}.\\] The set \\(\\mathcal{T}\\) is sometimes called the natural parameter space. The canonical form of an exponential family is easy to work with mathematically. Thus, most theorems about exponential families are expressed in canonical form.\nWritten in canonical form, the terms \\(\\eta, T(x), \\psi(\\eta),\\) and \\(h(x)\\) have special names:\n\\(\\eta\\) is called the canonical (or natural) parameter,\n\\(T(x)\\) is called the sufficient statistic,\n\\(\\psi(\\eta)\\) is called the cumulant-generating function, and\n\\(h(x)\\) is called the carrying density.\nVocabulary can be a bit annoying, but in the case of exponential families it facilitates discussion. We look at a couple examples of exponential families in canonical form.\nExample: Poisson canonical form. Recall that we can write the Poisson density in exponential family form as \\[f(x | \\theta) = \\left(e^{x \\log(\\theta)} - \\theta \\right)\\frac{1}{x!}.\\] Set \\(\\eta^* = \\log(\\theta)\\). Then \\(\\theta = e^{\\eta^*},\\) and we can re-express \\(f\\) as \\[f(x| \\eta^*) = \\left(e^{x \\eta^* - e^{\\eta^*}} \\right) \\frac{1}{x!}.\\] Dropping the asterisk from \\(\\eta\\) to ease notation, we end up with the canonical parameterization \\[f(x| \\eta) = \\left(e^{x \\eta - e^{\\eta}}\\right) \\frac{1}{x!},\\] where \\(\\eta \\in \\mathcal{T} = \\mathbb{R}.\\) The canonical parameter is \\(\\eta\\), the sufficient statistic is \\(x\\), the cumulant-generating function is \\(e^\\eta\\), and the carrying density is \\(1/x!\\).\nExample: Negative binomial canonical form. We expressed the negative binomial density in exponential family form as \\[ f(x|\\theta) = e^{ \\log (\\theta)x  + r \\log(1 - \\theta)} \\binom{x + r - 1}{x}.\\] Setting \\(\\eta = \\log(\\theta)\\), we can re-write this density in canonical form as \\[ f(x|\\eta) = e^{x \\eta + r \\log(1-e^\\eta)}\\binom{x+r-1}{x}.\\] The canonical parameter is \\(\\eta\\), the sufficient statistic is \\(x\\), the cumulant-generating function is \\(r\\log(1 - e^\\eta)\\), and the carrying density is \\(\\binom{x + r - 1}{x}.\\)\nConvexity\nThe exponential family enjoys some useful convexity properties.\nTheorem: Consider a canonical exponential family with density \\[f(x|\\eta) = e^{\\eta T(x) - \\psi(\\eta)}h(x)\\] and natural parameter space \\(\\mathcal{T}\\). The set \\(\\mathcal{T}\\) is convex, and the cumulant-generating function \\(\\psi\\) is convex on \\(\\mathcal{T}\\).\nThe proof of this theorem is simple and involves the application of Holder’s inequality. This theorem has an important corollary.\nCorollary: Let \\(X = (X_1, \\dots, X_d)\\) be a random vector distributed according to the exponential family \\(\\{ P_\\eta : \\eta \\in \\mathcal{T}\\}.\\) The log-likelihood \\[\\mathcal{L}(\\eta; x) = \\log\\left(f(x|\\eta)\\right)\\] is a concave function defined on a convex set.\nThe proof of this corollary is simple. Because \\(\\psi\\) is convex, \\(-\\psi\\) is concave. The log-likelihood \\[\\mathcal{L}(\\eta;x) = \\eta T(x) - \\psi(\\eta) + \\log\\left( h(x) \\right)\\] is the sum of concave functions and is therefore concave.\nThis corollary has important implications: the MLE for \\(\\eta\\) exists and is easily computable (through convex optimization). When \\(\\psi\\) is strictly convex (which generally is the case), the MLE is unique.\nMoments\nWe easily can compute the moments of an exponential family.\nTheorem. Let \\(X = (X_1, \\dots, X_d)\\) be distributed according to the canonical exponential family \\(\\{ P_\\eta:\\eta \\in \\mathcal{T}\\}.\\) Then \\[\n\\begin{cases}\n\\mathbb{E}_\\eta [T(X)] = \\psi'(\\eta) \\\\\n\\mathbb{V}_\\eta [T(X)] = \\psi''(\\eta).\n\\end{cases}\n\\]\nWe provide a proof sketch. The density function of the expoential family integrates to unity, i.e. \\[ \\int_{\\mathbb{R}^d} e^{\\eta T(x) - \\psi(\\eta)}h(x) d\\alpha(x) = 1.\\] Therefore, \\[ e^{\\psi(\\eta)} = \\int_{ \\mathbb{R}^d } e^{\\eta T(x)}h(x) d\\alpha(x).\\] Differentiating with respect to \\(\\eta\\), we find \\[ e^{\\psi(\\eta)} \\psi'(\\eta) = \\int_{\\mathbb{R}^d} T(x)e^{\\eta T(x)} h(x) d\\alpha(x),\\] and so \\[ \\psi'(\\eta) = \\int_{\\mathbb{R}^d} T(x) e^{\\eta T(x) - \\psi(\\eta)}h(x) d\\alpha(x) = \\int_{\\mathbb{R}^d} T(x) f(x|\\eta) d\\alpha(x) = \\mathbb{E}_\\eta[T(X)].\\] Differentiating with respect to \\(\\eta\\) again and rearranging, we find \\(\\mathbb{V}_\\eta [T(X)] = \\psi''(\\eta)\\).\nExample: Recall that the Poisson distribution can be written in canonical form as \\[ f(x|\\eta) = e^{x \\eta - e^{\\eta}} \\frac{1}{x!}.\\] We have that \\(\\psi(\\eta) = e^{\\eta}\\) and \\(T(X) = X\\). Therefore, \\(\\mathbb{E}_\\eta[X] = \\psi'(\\eta) = e^{\\eta}\\) and \\(\\mathbb{V}_\\eta[X] = \\psi''(\\eta) = e^{\\eta}.\\) Recalling that \\(\\eta = \\log(\\theta)\\), we recover \\(\\mathbb{E}[X] = \\mathbb{V}[X] = \\theta\\).\nOur theorem about the moments of \\(T(X)\\) has several intriguing corollaries.\nCorollary 1. The second derivative of \\(\\psi\\) is equal to the variance of \\(T(X)\\). Because variance is non-negative, the second derivative of \\(\\psi\\) is non-negative. This implies that \\(\\psi\\) is convex. This is an alternate demonstration of the convexity of \\(\\psi\\).\nCorollary 2. Assume the variance of \\(T(X)\\) is nonzero. Then \\(\\psi\\) is strictly convex, implying \\(\\eta \\to \\mathbb{E}_\\eta[T(X)]\\) is injective. Thus, we can reparameterize the exponential family in terms of \\(\\mathbb{E}_\\eta[T(X)].\\) Because \\(T(X) = X\\) for the Poisson and negative binomial distributions in particular, we can parameterize the Poisson and negative binomial densities in terms of their means.\nMultiparameter exponential family\nWe extend the definition of the exponential family to multiparameter distributions. Results that hold for one-parameter exponential families hold analogously for multiparameter exponential families.\nDefinition\nLet the random vector \\(X = (X_1, \\dots, X_d)\\) have distribution \\(P_\\theta,\\) where \\(\\theta \\in \\Theta \\subset \\mathbb{R}^k\\). The family \\(\\{P_\\theta \\}\\) belongs to the \\(k\\)-parameter exponential family if its density can be written as \\[f(x|\\theta) = e^{ \\sum_{i=1}^k \\eta_i(\\theta)T_i(x) - \\psi(\\theta)}h(x),\\] where \\[ \\begin{cases}\n\\eta_1, \\dotsm \\eta_k : \\mathbb{R}^k \\to \\mathbb{R} \\\\\nT_1, \\dots, T_k : \\mathbb{R}^d \\to \\mathbb{R} \\\\\n\\psi : \\mathbb{R}^k \\to \\mathbb{R} \\\\\nh : \\mathbb{R}^d \\to \\mathbb{R}\n\\end{cases}\n\\] are functions, and \\(\\theta \\in \\mathbb{R}^k\\). We also require that the dimension of \\(\\theta = (\\theta_1, \\dots, \\theta_n)\\) equal the dimension of \\((\\eta_1(\\theta), \\dots, \\eta_k(\\theta))\\). (If the dimension of the latter exceeds that of the former, the distribution is said to belong to the curved exponential family.)\nExamples\nExamples of the multiparameter exponential family include the normal distribution with unknown mean and variance and generalized linear models (GLMs). We will provide more detailed examples of multiparameter exponential families in the upcoming post on GLMs.\nProperties\nWe briefly list some properties of the multiparameter exponential family related to sufficiency, reparameterization, convexity, and moments.\nSufficiency\nThe vector \\([T_1(X), \\dots, T_k(X)]^T\\) is a sufficient statistic for the parameter \\(\\theta\\). This follows from the Neyman-Fisher factorization theorem.\nReparameterization\nWe can reparameterize multivariate distributions as well. Similar to the one-parameter case, let \\[ \\begin{cases}\n\\eta_1^* = \\eta_1(\\theta_1, \\dots, \\theta_k) \\\\\n\\dots \\\\\n\\eta_k^* = \\eta_k(\\theta_1, \\dots, \\theta_k).\n\\end{cases}\n\\] Typically, we can invert this vector-valued function, i.e., we can express \\(\\theta_1, \\dots, \\theta_k\\) in terms of \\(\\eta_1^*, \\dots, \\eta_k^*\\). In this case, we can write the multiparameter exponential family in canonical form: \\[f(x|\\eta) = e^{ \\sum_{i=1}^k \\eta_i T_i(x) - \\psi(\\eta)}h(x),\\] where \\(\\eta \\in \\mathbb{R}^k, T_1, \\dots T_k: \\mathbb{R}^d \\to \\mathbb{R},\\) \\(\\psi: \\mathbb{R}^k \\to \\mathbb{R},\\) and \\(h:\\mathbb{R}^d \\to \\mathbb{R}\\). The set \\(\\mathcal{T}\\) over which the natural parameters vary is called the natural parameter space.\nConvexity\nThe natural parameter space \\(\\mathcal{T}\\) is convex, and the function \\(\\psi: \\mathbb{R}^k \\to \\mathbb{R}\\) is convex over \\(\\mathcal{T}\\). Thus, the log-likelihood for the natural parameter \\(\\eta\\) of a \\(k\\)-parameter exponential family is concave. The proof of this assertion in multiparameter families likewise leverages Holder’s inequality.\nMoments\nLet \\(X = (X_1, \\dots, X_d)\\) have distribution \\(\\{P_\\eta\\}\\) belonging to the canonical \\(k-\\)parameter exponential family. Then \\[ \\nabla \\psi(\\eta) = \\mathbb{E}_\\eta[ T_1(X), \\dots, T_k(X) ] \\] and \\[ \\nabla^2 \\psi(\\eta) = \\textrm{Cov}_\\eta[T_1(X), \\dots, T_k(X)],\\] where \\(\\nabla \\psi\\) is the gradient of \\(\\psi\\), and \\(\\nabla^2\\psi\\) is the Hessian of \\(\\psi\\). In words, the gradient of the cumulant-generating function \\(\\psi\\) is the expected value of the vector of sufficient statistics \\([T_1(X), \\dots, T_k(X)]\\), and the hessian of \\(\\psi\\) is the variance-covariance matrix of the vector of sufficient statistics. Because variance-covariance matrices are positive semi-definite, the function \\(\\psi\\) is convex. This is an alternate demonstration of the convexity of \\(\\psi\\). The Hessian of \\(\\psi\\) evaulated at \\(\\eta\\) sometimes is called the Fisher information matrix (evaluated at \\(\\eta\\)).\nConclusion\nThe exponential family is a mathematical abstraction that unifies common parametric probability distributions. In this post we defined exponential families and explored some of their basic properties. In the remaining two posts of this mini-series, we will explore the connection between exponential families, information matrices, and GLMs.\n\n\nme.me\n\nReferenes\nLecture notes provided by Professor Anirban DasGupta.\n",
    "preview": "posts/2020-06-22-exponential-families/happy_normal.jpg",
    "last_modified": "2021-05-21T21:30:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-16-gamma-poisson-nb/",
    "title": "Gamma, Poisson, and negative binomial distributions",
    "description": "The gamma, Poisson, and negative binomial distributions are used extensively in genomics. In this post we review these distributions and their connections to one another. We also cover the various (and somewhat confusing) parameterizations of these distributions.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-06-16",
    "categories": [
      "Statistics"
    ],
    "contents": "\nThe gamma, Poisson, and negative binomial distributions frequently are used to model RNAseq and single cell RNAseq data. Here, we review the statistical properties of these distributions. We postpone discussion of modeling RNAseq data to a subsequent post.\nGamma\nThe gamma distribution is a non-negative, continuous, two-parameter probability distribution. There are two common parameterizations of the gamma distribution: the “shape-scale” parameterization and the “shape-rate” parameterization. The pdf of the gamma distribution under the former parameterization is \\[f(x;k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} e^{-\\frac{x}{\\theta}},\\] where \\(k > 0\\) is the shape parameter, \\(\\theta > 0\\) is the scale parameter, and \\(\\Gamma\\) is the gamma function. Recall that, for positive real numbers, the gamma function is defined by \\[ \\Gamma(x) = \\int_{0}^\\infty t^{-x}e^{-t} dt.\\] The Gamma distribution has support \\((0,\\infty)\\). The mean and variance of the Gamma distribution (under shape-scale parameterization) are \\(k\\theta\\) and \\(k\\theta^2\\). The alternate parameterization of the gamma distribution — the shape-rate parameterization — sets \\(\\alpha = k\\) and \\(\\beta = 1/\\theta\\). The parameter \\(\\alpha\\) retains the name of the shape parameter \\(k\\), and \\(\\beta\\) is called the scale parameter.\nPoisson\nThe Poisson distribution is a discrete probability distribution used to model (non-negative) count data. The pmf of the Poisson distribution is \\[ p(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!},\\] where \\(\\lambda > 0\\) is called the rate parameter. The support of the distribution is \\(\\mathbb{Z}^{\\geq 0}\\), and the mean and variance are \\(\\lambda\\). The Poisson and Gamma distributions are members of the exponential family, and so parameter estimation (through, e.g., MLE) is simple in both models.\nNegative binomial\nThe negative binomial (NB) distribution is a discrete probability distribution that takes support on the non-negative integers. The NB distribution models the number of failures in a sequence of independent trials before a specified number of successes occurs. For example, suppose we flip a coin repeatedly until we see 10 heads. The total number of tails we see throughout the experiment follows an NB distribution. The pmf of the NB distribution (under standard parameterization) is \\[ p(x; r, p) = \\binom{x + r - 1}{x} (1 - p)^r p^x, \\] where \\(r \\in \\mathbb{Z}^{\\geq 0}\\) is the number of successes, and \\(p \\in [0,1]\\) is the probability of failure. The mean and variance of the NB distribution are \\[\\frac{pr}{1-p}\\] and \\[\\frac{1 + p}{(1-p)^2}.\\] The pmf of the NB distribution can be generalized to allow for \\(r \\in \\mathbb{R}^{\\geq 0}\\) as follows: \\[ p(x; r, p) = \\frac{\\Gamma(x + r)}{x! \\Gamma(r)}(1-p)^rp^k.\\] This extension follows from properties of the gamma function.\nIf one holds \\(r\\) fixed, the NB distribution is a member of the exponential family. However, when both \\(r\\) and \\(p\\) are allowed to vary, the NB distribution is not member of the exponential family. This means that parameter estimation in the two-parameter NB model is a bit more challenging than parameter estimation in many other common parameteric models. In particular, the NB maximum likelihood estimate fails to exist when the sample mean exceeds the sample seond moment (Aragon, Eberly, and Eberly 1992) .\nGamma-Poisson mixture\nA very cool (and useful) fact about the negative binomial distribution is that it can be written as a mixture of gamma and Poisson distributions. Consider a Poisson model with gamma-distributed mean: \\[\n\\begin{cases}\nY \\sim \\textrm{Pois}(\\theta) \\\\\n\\theta \\sim \\textrm{gamma}(r, \\frac{p}{1-p}),\n\\end{cases}\n\\] where the gamma distribution is expressed in shape-scale parameterization. One can show that \\(Y\\) is negative binomially distributed with parameters \\(r\\) and \\(p\\), i.e. \\[ Y \\sim \\textrm{NB}(r,p).\\] A quick proof (taken from this blog post) is as follows: \\[\n\\begin{align*}\np(y; r, p) = \\int_{0}^{\\infty} p(y | \\theta) p(\\theta) d \\theta \\textrm{ (law of total probability)} \\\\\n= \\int_{0}^{\\infty} \\left( \\frac{\\theta^y e^{-\\theta}}{y!} \\right) \\left( \\frac{ \\theta^{r-1} e^{-\\theta (1 - p)/p} }{ \\Gamma(r) \\left(\\frac{p}{1-p}\\right)^r}\\right) d\\theta \\textrm{ (plug in pdfs)} \n\\\\ = \\frac{\\Gamma(y + r)}{y! \\Gamma(r)}(1-p)^r p^y \\textrm{ (compute integral)} \\\\ \\sim NB(r,p).\n\\end{align*}\n\\]\nParameterizations of the negative binomial distribution\nThere are several parameterizations of the negative binomial distribution. We list three such parameterizations in the table below. The left column shows the (shape-scale) parameterization of the underlying gamma distribution. The middle and right columns show the parameterization and pmf of the corresponding NB distribution.\nIdx\nGamma parameterization\nNB parameterization\nNB pmf\n1.\nGamma\\(\\left(r, \\frac{p}{1 - p}\\right)\\)\nNB\\(\\left(r,p\\right)\\)\n\\[\\binom{x + r - 1}{x} p^x (1-p)^r \\]\n2.\nGamma\\(\\left(r, \\frac{p}{r} \\right)\\)\nNB\\(\\left(r, \\frac{p}{p+r}\\right)\\)\n\\[ \\binom{x + r - 1}{x} \\left( \\frac{p}{p+r} \\right)^x \\left( \\frac{r}{p+r} \\right)^r  \\]\n3.\nGamma\\(\\left(r, p \\right)\\)\nNB\\(\\left(r, \\frac{p}{p+1}\\right)\\)\n\\[ \\binom{x + r - 1}{x} \\left( \\frac{p}{p+1} \\right)^x\\left( \\frac{1}{p+1} \\right)^r \\]\nOf these three NB parameterizations, the most common are the first and second. The first is similar (but not identical) to the default used by the various NB functions (dnbinom, rnbinom, etc.) in base R (see documentation). The second is the default used by the function negbinomial in the popular R package VGAM. Note that in the second and third parameterizations, \\(p\\) takes values in \\(\\mathbb{R}^{\\geq 0}\\) and therefore cannot be interpreted as a probability.\nNB, Poisson, and overdispersion\nConsider the second parameterization of the NB distribution in the table above. Its mean is \\[ \\frac{p/(p+r)r}{1 -p/(p+r)} = p\\] and its variance is \\[ \\frac{ p/(p+r) r }{ (1 - p/(p+r))^2 } = p + \\frac{p^2}{r}.\\] Thus, for \\(r < \\infty\\), the variance of the negative binomial distribution exceeds the mean. This is an important difference between the Poisson and NB distributions — in the Poisson distribution, the mean and variance coincide. One can show that the NB distribution converges to the Poisson distribution as \\(r\\) tends to infinity. Informally, \\[\\lim_{r \\to \\infty} \\textrm{NB}\\left( r, \\frac{\\lambda}{r + \\lambda}\\right) = \\textrm{Pois}(\\lambda).\\] Thus, for large \\(r\\), the NB and Poisson distributions are approximately equivalent. For small \\(r\\), the NB distribution is a sort of “overdispersed” Poisson distribution. This property of the NB distribution makes it an appealing candidate for modeling highly variable gene expression data.\n\n\n\nAragon, Jorge, David Eberly, and Shelly Eberly. 1992. “Existence and uniqueness of the maximum likelihood estimator for the two-parameter negative binomial distribution.” Statistics and Probability Letters 15 (5): 375–79. https://doi.org/10.1016/0167-7152(92)90157-Z.\n\n\n\n\n",
    "preview": "posts/2020-06-16-gamma-poisson-nb/picsvg.png",
    "last_modified": "2021-05-21T21:30:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-27-enhancers/",
    "title": "Genomics for statisticians 1: Enhancers",
    "description": "Enhancers are segments of DNA that increase the expression of a nearby gene. This post provides an overview enhancer biology and detection of enhancers at scale.",
    "author": [
      {
        "name": "Tim Barry",
        "url": "https://timothy-barry.github.io"
      }
    ],
    "date": "2020-05-27",
    "categories": [
      "Genomics"
    ],
    "contents": "\nTable of Contents\nIntroduction\nReview of chromatin and transcription\nReview of enhancer biology\nMethods for enhancer identification at scale\nCRISPR-based methods for enhancer identification at scale\nReferences\nThis is the first post in a series called Genomics for Statisticians. In this series I will explore some important ideas in modern genomics from the perspective of a non-biologist. When I began research in statistical genomics, I quickly discovered that my knowledge of genomics was not quite up to par. This series is the result of my effort to fill that gap. I hope other researchers in genomics who do not have extensive formal training in biology (e.g., statisticians, computer scientists) will find this series helpful as well. The posts should be reasonably self-contained. My goal is to explore concepts at the level of a college biology class, roughly. The first post in the series in on enhancers.\nIntroduction\nEnhancers are short regions of DNA that increase the expression of a nearby gene.\nMost variants responsible for heritability fall outside genes and in enhancers. Therefore, enhancers are important to study and understand.\nThis post has three parts. First, we review necessary background information on chromatin and transcription. Second, we review enhancer biology. Finally, we review methods for identifying enhancers and linking enhancers to genes at scale.\nReview of chromatin and transcription\nStructure of DNA, chromatin, and chromosomes\nDNA is a double-stranded molecule that stores genetic information. A strand of DNA consists of a bunch of linked nucleotides. Nucleotides come in four types: A, T, G, C.\nDNA molecules do not exist on their own in the nuclei of cells. Instead, they are packaged with proteins called histones to form chromatin. Put simply, chromatin = DNA + histones.\nThere are 5 histone proteins: H1, H2A, H2B, H3, and H4.\nA nucleosome is a chromatin structure consisting of a DNA strand wrapped twice around a nucleosome core, anchored in place by an H1 protein. A nucleosome core consists of two copies each of H2A, H2B, H3, and H4.\n\n\nA nucleosome, which consists of an 8-histone core, a strand of DNA wrapped twice around the histone core, and an H1 histone to help anchor the DNA in place.\n\n\nNucleosomes pack together in a highly organized way to form chromosomes.\n\n\n\nNucleosomes are arranged to form chromosomes.\n\n\nChanges in chromatin structure\nDNA in tightly-coiled chromatin cannot be accessed by proteins.\nChromatin can “loosen up,” thereby exposing DNA to proteins.\nChromatin “loosens” or “tightens” by three main processes:\nChromatin remodeling: Proteins called chromatin remodeling complexes physically reposition nucleosomes to expose DNA.\n\nHistone modification: Histones are chemically altered by addition or removal of functional groups.\nExample: an acetyl group (CH\\(_3\\)CO) can be added to the 27th lysine amino acid of H3 to loosen the chromatin. In chemical notation, we write H3 \\(\\to\\) H3K27ac (K stands for “lysine,” and “ac” stands for acetyl group).\nExample: a methyl group (CH\\(_3\\)) can be added to the 4th lysine amino acid of H3 to loosen the chromatin. In chemical notation, we write H3 \\(\\to\\) H3K4me1 (“me1” stands for methyl group).\n\nDNA methylation: Cytosine bases of DNA also can be methylated to form 5-methylcytosine. Heavily methylated DNA is generally not transcribed.\n\n\nTranscription of genes\nTranscription is the copying of a gene into a complimentary RNA molecule.\nA promoter is a short sequence of DNA upstream of a gene that helps initiate transcription.\nThe protein polymerase and other proteins called transcription factors and transcription activator proteins bind to the promoter.\nLike a train traveling down railroad tracks, polymerase travels from the promoter into the gene and synthesizes a complimentary strand of RNA.\nReview of enhancer biology\nWhat is an enhancer?\nAn enhancer is a short sequence of DNA that increases the transcription of one or more nearby genes through physical contact (known as a cis-regulatory mechanism).\n\nHow do enhancers work?\nEnhancers bind transcription factors and transcription activator proteins and bring these proteins close (in physical space) to the promoter of the target gene. This action modulates the expression of the target gene.\nEnhancers can regulate expression of their target gene in other ways. For example, some enhancers regulate the release of paused polymerase, and others act through RNA splicing mechanisms.\n\n\n\n\nAn enhancer modulating the expression of a gene.\n\n\nWhat are some features of enhancers?\nEnhancers are located in regions of open (or “loose”) chromatin.\nEnhancers are flanked by histones carrying H3K27ac and H3K4me1 modifications.\nEnhancers are flanked by methylated DNA.\nEnhancers are a few hundred base pairs in length and act over \\(\\approx\\) 30,000 bases.\nGenes can be affected by a single enhancer or multiple, “interacting” enhancers; conversely, individual enhancers can regulate multiple genes.\nEnhancers can reside in clusters of hundreds of enhancers, forming “super enhancers.”\n\nMethods for enhancer identification at scale\nDNA sequence analysis: Enhancers harbor transcription factor binding motifs (i.e., very short stretches of DNA that help bind transcription factors). Furthermore, enhancers are often conserved across species. Thus, we can try to predict whether a region of the genome is an enhancer simply by looking at the corresponding primary sequence.\nBiochemical annotations: As we have discussed, several biochemical annotations correlate with enhancer activity. Genome-wide, we can search for (i) histone modifications (e.g., presence H3K27ac and H3K4me1), (ii) transcription factor binding, (iii) open chromatin, (iv) DNA methylation, and (v) the initiation of transcription. Many assays, both bulk-tissue and single-cell, provide us with this information (see, for example, DNase-seq, Pro-seq, and ChIP-seq).\neQTL mapping: For a given SNP and given nearby gene, we can test whether expression of that gene differs significantly across the levels of the SNP. If so, the SNP may lie within an enhancer for that gene. A downside of this approach is that it operates at the resolution of linkage disequilibrium blocks.\n3D conformation mapping: There exist assays (e.g., Hi-C) to probe the 3D conformation (in space) of a chromosome. If a given region of DNA is in close proximity to a known promoter, then that region might be an enhancer.\nCRISPR-based approaches: See next section.\nCRISPR-based methods for enhancer identification at scale\nCRISPR-Cas9 is a flexible technology for genome perturbation and editing.\nThe basic idea is to use CRISPR to “perturb” (in some way) a short sequence of DNA. If this perturbation is associated with a shift in the expression of a nearby gene, then the targeted sequence of DNA is likely an enhancer for that gene.\nCRISPR allows us to link enhancers to their target genes and (possibly) establish rigorous causal relationships between enhancer activity and gene expression.\nCRISPR-based approaches differ along two main axes:\nNumber of genes measured: Some CRISPR-based protocols track only a single gene; others track all genes (through, e.g., scRNA-seq).\n\nMode of perturbation: CRISPR can “perturb” a candidate enhancer in several ways:\nCRISPR–Cas9 (nuclease active) makes a single cut inside the enhancer. The cell uses non-homologous end joining to repair the cut. Generally, a few extra bases are inserted or deleted during this process, altering and consequently deactivating the targeted enhancer.\nCRISPRi (short for CRISPR interference) “turns off” a candidate enhancer without altering its associated DNA sequence.\n\n\nReferences\nToward a comprehensive catalog of validated and target-linked human enhancers (Gasperini, Tome, and Shendure 2020).\nGenetics: A conceptual approach (Pierce 2014).\nImage 1 source.\nImage 2 source.\nImage 3 source.\n",
    "preview": "posts/2020-05-27-enhancers/Nucleosome.jpg",
    "last_modified": "2021-05-21T21:30:06-04:00",
    "input_file": {}
  }
]
